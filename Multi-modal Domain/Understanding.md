
- **[1] Accelerating Transformers with Spectrum-Preserving Token Merging**, NeurIPS 2024.
  
  *Tran, Hoai-Chau and Nguyen, Duy MH and Nguyen, Duy M and Nguyen, Trung-Tin and Le, Ngan and Xie, Pengtao and Sonntag, Daniel and Zou, James Y and Nguyen, Binh T and Niepert, Mathias.*

  [[Paper](https://arxiv.org/abs/2405.16148)] [[Code](https://github.com/hchautran/PiToMe)] ![](https://img.shields.io/badge/PiToMe-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[2] LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models**, arXiv 2024.
  
  *Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan.*

  [[Paper](https://arxiv.org/abs/2403.15388)] [[Code](https://github.com/42Shawn/LLaVA-PruMerge)] ![](https://img.shields.io/badge/LLaVA_PruMerge-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[3] MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer**, CVPR 2024.
  
  *Cao, Jianjian and Ye, Peng and Li, Shengze and Yu, Chong and Tang, Yansong and Lu, Jiwen and Chen, Tao.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.html)] [[Code](https://github.com/double125/MADTP)] ![](https://img.shields.io/badge/MADTP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models**, arXiv 2024.
  
  *Ye, Xubing and Gan, Yukang and Ge, Yixiao and Zhang, Xiao-Ping and Tang, Yansong.*

  [[Paper](https://arxiv.org/abs/2412.00447)] [[Code](https://yxxxb.github.io/ATP-LLaVA-page/)] ![](https://img.shields.io/badge/ATP_LLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[5] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification**, arXiv 2024.
  
  *He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan.*

  [[Paper](https://arxiv.org/abs/2410.08584)] [Code] ![](https://img.shields.io/badge/ZipVL-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[6] HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models**, arXiv 2024.
  
  *Arif, Kazi Hasan Ibn and Yoon, JinYi and Nikolopoulos, Dimitrios S and Vandierendonck, Hans and John, Deepu and Ji, Bo.*

  [[Paper](https://arxiv.org/abs/2408.10945)] [[Code](https://github.com/hasanar1f/HiRED)] ![](https://img.shields.io/badge/HiRED-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[7] Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models**, arXiv 2024.
  
  *Ye, Weihao and Wu, Qiong and Lin, Wenhao and Zhou, Yiyi.*

  [[Paper](https://arxiv.org/abs/2409.10197)] [[Code](https://github.com/ywh187/FitPrune)] ![](https://img.shields.io/badge/FitPrune-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[8] Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving**, arXiv 2024.
  
  *Ma, Yunsheng and Abdelraouf, Amr and Gupta, Rohit and Wang, Ziran and Han, Kyungtae.*

  [[Paper](https://arxiv.org/abs/2409.11182)] [Code] ![](https://img.shields.io/badge/VTS-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Saliency_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[9] DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**, arXiv 2024.
  
  *Yao, Linli and Li, Lei and Ren, Shuhuai and Wang, Lean and Liu, Yuanxin and Sun, Xu and Hou, Lu.*

  [[Paper](https://arxiv.org/abs/2405.20985)] [[Code](https://github.com/yaolinli/DeCo)] ![](https://img.shields.io/badge/DeCo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[10] LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**, ECCV 2024.
  
  *Li, Yanwei and Wang, Chengyao and Jia, Jiaya.*

  [[Paper](https://arxiv.org/abs/2311.17043)] [[Code](https://github.com/dvlab-research/LLaMA-VID)] ![](https://img.shields.io/badge/LLaMA_VID-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[11] iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models**, arXiv 2024.
  
  *Hu, Lianyu and Shang, Fanhua and Wan, Liang and Feng, Wei.*

  [[Paper](https://arxiv.org/abs/2412.06263)] [Code] ![](https://img.shields.io/badge/iLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[12] VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation**, WACV 2025.
  
  *Chen, Hanning and Ni, Yang and Huang, Wenjun and Liu, Yezi and Jeong, SungHeon and Wen, Fei and Bastian, Nathaniel and Latapie, Hugo and Imani, Mohsen.*

  [[Paper](https://arxiv.org/abs/2409.08464)] [Code] ![](https://img.shields.io/badge/VLTP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[13] Efficient Multi-modal Large Language Models via Visual Token Grouping**, arXiv 2024.
  
  *Huang, Minbin and Huang, Runhui and Shi, Han and Chen, Yimeng and Zheng, Chuanyang and Sun, Xiangguo and Jiang, Xin and Li, Zhenguo and Cheng, Hong.*

  [[Paper](https://arxiv.org/abs/2411.17773)] [Code] ![](https://img.shields.io/badge/VisToG-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[14] CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers**, ICML 2024.
  
  *Shi, Dachuan and Tao, Chaofan and Rao, Anyi and Yang, Zhendong and Yuan, Chun and Wang, Jiaqi.*

  [[Paper](https://arxiv.org/abs/2305.17455)] [Code] ![](https://img.shields.io/badge/CrossGET-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Image_Captioning-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/CrossModal_Based-purple) ![](https://img.shields.io/badge/Token_Ensemble-orange)

- **[15] Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information**, AAAI 2025.
  
  *Chen, Yi and Xu, Jian and Zhang, Xu-Yao and Liu, Wen-Zhuo and Liu, Yang-Yang and Liu, Cheng-Lin.*

  [[Paper](https://arxiv.org/abs/2409.01179)] [[Code](https://github.com/banjiuyufen/RecoverableCompression)] ![](https://img.shields.io/badge/Recoverable_Compression-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[16] SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference**, arXiv 2024.
  
  *Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others.*

  [[Paper](https://arxiv.org/abs/2410.04417)] [[Code](https://github.com/Gumpest/SparseVLMs)] ![](https://img.shields.io/badge/SparseVLM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[17] PuMer: Pruning and Merging Tokens for Efficient Vision Language Models**, ACL 2023.
  
  *Cao, Qingqing and Paranjape, Bhargavi and Hajishirzi, Hannaneh.*

  [[Paper](https://arxiv.org/abs/2305.17530)] [[Code](https://github.com/csarron/PuMer)] ![](https://img.shields.io/badge/PuMer-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[18] Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**, arXiv 2024.
  
  *Song, Dingjie and Wang, Wenjun and Chen, Shunian and Wang, Xidong and Guan, Michael and Wang, Benyou.*

  [[Paper](https://arxiv.org/abs/2409.10994)] [[Code](https://github.com/FreedomIntelligence/TRIM/)] ![](https://img.shields.io/badge/LessIsMore-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[19] Multimodal Token Fusion for Vision Transformers**, CVPR 2022.
  
  *Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe.*

  [[Paper](https://arxiv.org/abs/2204.08721)] [Code] ![](https://img.shields.io/badge/TokenFusion-blue) ![](https://img.shields.io/badge/Image_to_Image_Translation-green) ![](https://img.shields.io/badge/RGB_Depth_Semantic_Segmentation-green) ![](https://img.shields.io/badge/Point_Cloud_3D_Object_Detection-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange) 

- **[20] Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model**, arXiv 2024.
  
  *Liu, Ting and Shi, Liangtao and Hong, Richang and Hu, Yue and Yin, Quanjun and Zhang, Linfeng.*

  [[Paper](https://arxiv.org/abs/2411.10803)] [[Code](https://github.com/liuting20/MustDrop)] ![](https://img.shields.io/badge/MUSTDrop-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[21] DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models**, arXiv 2024.
  
  *Tao, Keda and Qin, Can and You, Haoxuan and Sui, Yang and Wang, Huan.*

  [[Paper](https://arxiv.org/abs/2411.15024)] [[Code](https://github.com/KD-TAO/DyCoke)] ![](https://img.shields.io/badge/DyCoke-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[22] AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning**, arXiv 2024.
  
  *Zhong, Yiwu and Liu, Zhuoming and Li, Yin and Wang, Liwei.*

  [[Paper](https://arxiv.org/abs/2412.03248)] [[Code](https://github.com/LaVi-Lab/AIM)] ![](https://img.shields.io/badge/AIM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Adaptive_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[23] [CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster**, arXiv 2024.
  
  *Zhang, Qizhe and Cheng, Aosong and Lu, Ming and Zhuo, Zhiyong and Wang, Minqi and Cao, Jiajun and Guo, Shaobo and She, Qi and Zhang, Shanghang.*

  [[Paper](https://arxiv.org/abs/2412.01818)] [[Code](https://github.com/Theia-4869/FasterVLM)] ![](https://img.shields.io/badge/FasterVLM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[24] ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens**, arXiv 2024.
  
  *Guo, Yangyang and Zhang, Haoyu and Wong, Yongkang and Nie, Liqiang and Kankanhalli, Mohan.*

  [[Paper](https://arxiv.org/abs/2309.16738)] [[Code](https://github.com/guoyang9/ELIP)] ![](https://img.shields.io/badge/ELIP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Image_Captioning-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)
  
- **[25] FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression**, arXiv 2024.
  
  *Zhu, Yuke and Xie, Chi and Liang, Shuang and Zheng, Bo and Guo, Sheng.*

  [[Paper](https://arxiv.org/abs/2411.14228)] [Code] ![](https://img.shields.io/badge/FocusLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[26] Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration**, arXiv 2024.
  
  *Han, Yuhang and Liu, Xuyang and Ding, Pengxiang and Wang, Donglin and Chen, Honggang and Yan, Qingsen and Huang, Siteng.*

  [[Paper](https://arxiv.org/abs/2411.17686)] [Code] ![](https://img.shields.io/badge/FiCoCo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[27] Accelerating MLLMs by Searching Optimal Vision Token Reduction**, arXiv 2024.
  
  *Zhao, Shiyu and Wang, Zhenting and Juefei-Xu, Felix and Xia, Xide and Liu, Miao and Wang, Xiaofang and Liang, Mingfu and Zhang, Ning and Metaxas, Dimitris N and Yu, Licheng.*

  [[Paper](https://arxiv.org/abs/2412.00556)] [Code] ![](https://img.shields.io/badge/G_Search-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[28] Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See**, arXiv 2024.
  
  *Zhang, Zeliang and Pham, Phu and Zhao, Wentian and Wan, Kun and Li, Yu-Jhe and Zhou, Jianing and Miranda, Daniel and Kale, Ajinkya and Xu, Chenliang.*

  [[Paper](https://arxiv.org/abs/2410.06169)] [[Code](https://github.com/ZhangAIPI/YOPO_MLLM_Pruning)] ![](https://img.shields.io/badge/VisualAsText-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Head_Pruning-orange)

- **[29] An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models**, ECCV 2024.
  
  *Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao.*

  [[Paper](https://arxiv.org/pdf/2403.06764)] [[Code](https://github.com/pkunlp-icler/FastV)] ![](https://img.shields.io/badge/FastV-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[30] Is Less More? Exploring Token Condensation as Training-free Adaptation for CLIP**, arXiv 2024.
  
  *Wang, Zixin and Gong, Dong and Wang, Sen and Huang, Zi and Luo, Yadan.*

  [[Paper](https://arxiv.org/abs/2410.14729)] [Code] ![](https://img.shields.io/badge/TokenCond-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[31] PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models**, arXiv 2024.
  
  *Wu, Tianxiang and Nie, Minxin and Cao, Ziqiang.*

  [[Paper](https://arxiv.org/abs/2410.07278)] [Code] ![](https://img.shields.io/badge/PAR-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[32] Inference Optimal VLMs Need Only One Visual Token but Larger Models**, arXiv 2024.
  
  *Li, Kevin Y and Goyal, Sachin and Semedo, Joao D and Kolter, J Zico.*

  [[Paper](https://arxiv.org/abs/2411.03312)] [[Code](https://github.com/locuslab/llava-token-compression)] ![](https://img.shields.io/badge/QueCC-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[33] HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models**, AAAI 2025.
  
  *Arif, Kazi Hasan Ibn and Yoon, JinYi and Nikolopoulos, Dimitrios S and Vandierendonck, Hans and John, Deepu and Ji, Bo.*

  [[Paper](https://arxiv.org/abs/2408.10945)] [[Code](https://github.com/hasanar1f/HiRED)] ![](https://img.shields.io/badge/HiRED-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[34] Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM**, arXiv 2024.
  
  *Wang, Han and Nie, Yuxiang and Ye, Yongjie and GuanYu, Deng and Wang, Yanjie and Li, Shuai and Yu, Haiyang and Lu, Jinghui and Huang, Can.*

  [[Paper](https://arxiv.org/abs/2412.09530)] [Code] ![](https://img.shields.io/badge/DynamicVLM-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[35] Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models**, ECCV 2024.
  
  *Ju, Chen and Wang, Haicheng and Li, Zeqian and Chen, Xu and Zhai, Zhonghua and Huang, Weilin and Xiao, Shuai.*

  [[Paper](https://arxiv.org/abs/2407.11717)] [Code] ![](https://img.shields.io/badge/Turbo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Image_Captioning-green) ![](https://img.shields.io/badge/Multi_Modal_Retrieval-green) ![](https://img.shields.io/badge/Text2Image-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[36] IVTP: Instruction-Guided Visual Token Pruning for Large Vision-Language Models**, ECCV 2024.
  
  *Huang, Kai and Zou, Hao and Xi, Ye and Wang, BoChen and Xie, Zhen and Yu, Liang.*

  [[Paper](https://arxiv.org/abs/2409.08464)] [Code] ![](https://img.shields.io/badge/IVTP-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[37] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval**, arXiv 2024.
  
  *Shen, Leqi and Hao, Tianxiang and Zhao, Sicheng and Zhang, Yifeng and Liu, Pengzhang and Bao, Yongjun and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2409.01156)] [Code] ![](https://img.shields.io/badge/TempMe-blue) ![](https://img.shields.io/badge/Text_Video_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[38] Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding**, arXiv 2024.
  
  *Zhang, Yiming and Zhao, Zhuokai and Chen, Zhaorun and Ding, Zenghui and Yang, Xianjun and Sun, Yining.*

  [[Paper](https://arxiv.org/abs/2411.14401)] [Code] ![](https://img.shields.io/badge/DYTO-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[39] PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models**, arXiv 2024.

  *Yang, Chenyu and Dong, Xuan and Zhu, Xizhou and Su, Weijie and Wang, Jiahao and Tian, Hao and Chen, Zhe and Wang, Wenhai and Lu, Lewei and and Dai, Jifeng.*

  [[Paper](https://arxiv.org/abs/2412.09613)] [[Code](https://github.com/OpenGVLab/PVC)] ![](https://img.shields.io/badge/PVC-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[40] ST<sup>3</sup>: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming**, AAAI 2025.

  *Jiedong Zhuang and Lu Lu and Ming Dai and Rui Hu and Jian Chen and Qiang Liu and Haoji Hu.*

  [[Paper](https://arxiv.org/abs/2412.20105)] [Code] ![](https://img.shields.io/badge/ST3-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[41] freePruner: A Training-free Approach for Large Multimodal Model Acceleration**, arXiv 2024.

  *Bingxin Xu and Yuzhang Shang and Yunhao Ge and Qian Lou and Yan Yan.*

  [[Paper](https://arxiv.org/abs/2411.15446)] [[Code](https://github.com/THU-MIG/VTC-CLS)] ![](https://img.shields.io/badge/freePruner-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[42] [CLS] Token Tells Everything Needed for Training-free Efficient MLLMs**, arXiv 2024.

  *Wang, Ao and Sun, Fengyuan and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2412.05819)] [[Code](https://github.com/THU-MIG/VTC-CLS)] ![](https://img.shields.io/badge/VTC_CLS-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[42] FoPru: Focal Pruning for Efficient Large Vision-Language Models**, arXiv 2024.

  *Lei Jiang and Weizhe Huang and Tongxuan Liu and Yuting Zeng and Jing Li and Lechao Cheng and Xiaohua Xu.*

  [[Paper](https://arxiv.org/abs/2411.14164)] [Code] ![](https://img.shields.io/badge/FoPru-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[43] FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance**, arXiv 2024.

  *Haicheng Wang and Zhemeng Yu and Gabriele Spadaro and Chen Ju and Victor Quétu and Enzo Tartaglione.*

  [[Paper](https://arxiv.org/abs/2501.02430)] [[Code](https://github.com/anakin-skywalker-Joseph/Folder)] ![](https://img.shields.io/badge/FOLDER-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[44] Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding**, arXiv 2024.

  *Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan and Liqiang Nie.*

  [[Paper](https://arxiv.org/abs/2407.14439)] [[Code](https://github.com/JiuTian-VL/TokenCorrCompressor)] ![](https://img.shields.io/badge/TokenCorrCompressor-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Document_Understanding-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[45] mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding**, Findings of EMNLP 2024.

  *Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others.*

  [[Paper](https://arxiv.org/abs/2403.12895)] [[Code](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5)] ![](https://img.shields.io/badge/mPLUG_DocOwl1.5-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Document_Understanding-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[46] mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding**, arXiv 2024.

  *Anwen Hu and Haiyang Xu and Liang Zhang and Jiabo Ye and Ming Yan and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou.*

  [[Paper](https://arxiv.org/abs/2409.03420)] [[Code](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2)] ![](https://img.shields.io/badge/mPLUG_DocOwl2-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Document_Understanding-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)
  
- **[47] LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**, arXiv 2025.

  *Shaolei Zhang and Qingkai Fang and Zhe Yang and Yang Feng.*

  [[Paper](https://arxiv.org/abs/2501.03895)] [[Code](https://github.com/ictnlp/LLaVA-Mini)] ![](https://img.shields.io/badge/LLaVA_Mini-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[48] What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph**, AAAI 2025.

  *Yutao Jiang and Qiong Wu and Wenhao Lin and Wei Yu and Yiyi Zhou.*

  [[Paper](https://arxiv.org/abs/2501.02268)] [[Code](https://github.com/ictnlp/LLaVA-Mini)] ![](https://img.shields.io/badge/LLaVA_Mini-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[49] FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models**, arXiv 2025.

  *Fu, Tianyu and Liu, Tengxuan and Han, Qinghao and Dai, Guohao and Yan, Shengen and Yang, Huazhong and Ning, Xuefei and Wang, Yu.*

  [[Paper](https://arxiv.org/abs/2501.01986)] [[Code](https://thu-nics.github.io/FrameFusion_Project_Page/)] ![](https://img.shields.io/badge/FrameFusion-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[50] Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration**, arXiv 2025.

  *Xuyang Liu and Ziming Wang and Yuhang Han and Yingyao Wang and Jiale Yuan and Jun Song and Bo Zheng and Linfeng Zhang and Siteng Huang and Honggang Chen.*

  [[Paper](https://arxiv.org/abs/2501.05179)] [[Code](https://github.com/xuyang-liu16/GlobalCom2)] ![](https://img.shields.io/badge/GlobalCom2-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[51] AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture**, arXiv 2025.
  
  *Jiayi Han and Liang Du and Yiwen Wu and Xiangguo Zhou and Hongwei Du and Weibo Zheng.*

  [[Paper](https://arxiv.org/abs/2501.09532)] [Code] ![](https://img.shields.io/badge/AdaFV-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[52] p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay**, arXiv 2024.
  
  *Zhang, Jun and Meng, Desen and Qi, Ji and Huang, Zhenpeng and Wu, Tao and Wang, Limin.*

  [[Paper](https://arxiv.org/abs/2412.04449)] [[Code](https://github.com/MCG-NJU/p-MoD)] ![](https://img.shields.io/badge/p_MoD-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)
  
