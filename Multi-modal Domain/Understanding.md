

- **[1] Accelerating Transformers with Spectrum-Preserving Token Merging**, NeurIPS 2024.
  
  *Tran, Hoai-Chau and Nguyen, Duy MH and Nguyen, Duy M and Nguyen, Trung-Tin and Le, Ngan and Xie, Pengtao and Sonntag, Daniel and Zou, James Y and Nguyen, Binh T and Niepert, Mathias.*

  [[Paper](https://arxiv.org/abs/2405.16148)] [[Code](https://github.com/hchautran/PiToMe)] ![](https://img.shields.io/badge/PiToMe-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[2] LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models**, arXiv 2024.
  
  *Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan.*

  [[Paper](https://arxiv.org/abs/2403.15388)] [[Code](https://github.com/42Shawn/LLaVA-PruMerge)] ![](https://img.shields.io/badge/LLaVA_PruMerge-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[3] MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer**, CVPR 2024.
  
  *Cao, Jianjian and Ye, Peng and Li, Shengze and Yu, Chong and Tang, Yansong and Lu, Jiwen and Chen, Tao.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.html)] [Code] ![](https://img.shields.io/badge/MADTP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models**, arXiv 2024.
  
  *Ye, Xubing and Gan, Yukang and Ge, Yixiao and Zhang, Xiao-Ping and Tang, Yansong.*

  [[Paper](https://arxiv.org/abs/2412.00447)] [Code] ![](https://img.shields.io/badge/ATP_LLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)


- **[5] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification**, arXiv 2024.
  
  *He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan.*

  [[Paper](https://arxiv.org/abs/2410.08584)] [Code] ![](https://img.shields.io/badge/ZipVL-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Sparsification-orange)

- **[6] HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models**, arXiv 2024.
  
  *Arif, Kazi Hasan Ibn and Yoon, JinYi and Nikolopoulos, Dimitrios S and Vandierendonck, Hans and John, Deepu and Ji, Bo.*

  [[Paper](https://arxiv.org/abs/2408.10945)] [Code] ![](https://img.shields.io/badge/HiRED-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Dropping-orange)

- **[7] Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models**, arXiv 2024.
  
  *Ye, Weihao and Wu, Qiong and Lin, Wenhao and Zhou, Yiyi.*

  [[Paper](https://arxiv.org/abs/2409.10197)] [Code] ![](https://img.shields.io/badge/FitPrune-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[8] Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving**, arXiv 2024.
  
  *Ma, Yunsheng and Abdelraouf, Amr and Gupta, Rohit and Wang, Ziran and Han, Kyungtae.*

  [[Paper](https://arxiv.org/abs/2409.11182)] [Code] ![](https://img.shields.io/badge/VTS-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Sparsification-orange)


- **[9] CATP: Cross-Attention Token Pruning for Accuracy Preserved Multimodal Model Inference**, arXiv 2024.
  
  *Liao, Ruqi and Zhao, Chuqing and Li, Jin and Feng, Weiqi.*

  [[Paper](https://arxiv.org/abs/2404.08567)] [Code] ![](https://img.shields.io/badge/CATP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[10] DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**, arXiv 2024.
  
  *Yao, Linli and Li, Lei and Ren, Shuhuai and Wang, Lean and Liu, Yuanxin and Sun, Xu and Hou, Lu.*

  [[Paper](https://arxiv.org/abs/2405.20985)] [Code] ![](https://img.shields.io/badge/DeCo-blue) ![](https://img.shields.io/badge/Text_to_Image-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)

- **[11] LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models**, ECCV 2025.
  
  *Li, Yanwei and Wang, Chengyao and Jia, Jiaya.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-72952-2_19)] [Code] ![](https://img.shields.io/badge/LLaMA_VID-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)


- **[12] iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models**, arXiv 2024.
  
  *Hu, Lianyu and Shang, Fanhua and Wan, Liang and Feng, Wei.*

  [[Paper](https://arxiv.org/abs/2412.06263)] [Code] ![](https://img.shields.io/badge/iLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)

- **[13] VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation**, arXiv 2024.
  
  *Chen, Hanning and Ni, Yang and Huang, Wenjun and Liu, Yezi and Jeong, SungHeon and Wen, Fei and Bastian, Nathaniel and Latapie, Hugo and Imani, Mohsen.*

  [[Paper](https://arxiv.org/abs/2409.08464)] [Code] ![](https://img.shields.io/badge/VLTP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/CrossModal_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[14] ToSA: Token Selective Attention for Efficient Vision Transformers**, arXiv 2024.
  
  *Singh, Manish Kumar and Yasarla, Rajeev and Cai, Hong and Lee, Mingu and Porikli, Fatih.*

  [[Paper](https://arxiv.org/abs/2406.08816)] [Code] ![](https://img.shields.io/badge/ToSA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Selection-orange)

- **[15] Efficient Multi-modal Large Language Models via Visual Token Grouping**, arXiv 2024.
  
  *Huang, Minbin and Huang, Runhui and Shi, Han and Chen, Yimeng and Zheng, Chuanyang and Sun, Xiangguo and Jiang, Xin and Li, Zhenguo and Cheng, Hong.*

  [[Paper](https://arxiv.org/abs/2411.17773)] [Code] ![](https://img.shields.io/badge/VTG-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Grouping-orange)


- **[16] CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers**, arXiv 2023.
  
  *Shi, Dachuan and Tao, Chaofan and Rao, Anyi and Yang, Zhendong and Yuan, Chun and Wang, Jiaqi.*

  [[Paper](https://arxiv.org/abs/2305.17455)] [Code] ![](https://img.shields.io/badge/CrossGET-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/CrossModal_Based-purple) ![](https://img.shields.io/badge/Token_Ensemble-orange)

- **[17] Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information**, arXiv 2024.
  
  *Chen, Yi and Xu, Jian and Zhang, Xu-Yao and Liu, Wen-Zhuo and Liu, Yang-Yang and Liu, Cheng-Lin.*

  [[Paper](https://arxiv.org/abs/2409.01179)] [Code] ![](https://img.shields.io/badge/RecComp-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Recovery-orange)

- **[18] SparseVLM: Visual Token Sparsification for Efficient Vision-Language Model Inference**, arXiv 2024.
  
  *Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others.*

  [[Paper](https://arxiv.org/abs/2410.04417)] [Code] ![](https://img.shields.io/badge/SparseVLM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Importance_Based-purple) ![](https://img.shields.io/badge/Token_Sparsification-orange)

- **[19] PuMer: Pruning and Merging Tokens for Efficient Vision Language Models**, ACL 2023.
  
  *Cao, Qingqing and Paranjape, Bhargavi and Hajishirzi, Hannaneh.*

  [[Paper](https://arxiv.org/abs/2305.17530)] [Code] ![](https://img.shields.io/badge/PuMer-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)


- **[20] Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**, arXiv 2024.
  
  *Song, Dingjie and Wang, Wenjun and Chen, Shunian and Wang, Xidong and Guan, Michael and Wang, Benyou.*

  [[Paper](https://arxiv.org/abs/2409.10994)] [[Code](https://github.com/FreedomIntelligence/TRIM/)] ![](https://img.shields.io/badge/LessIsMore-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Importance_Based-purple) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[21] Multimodal Token Fusion for Vision Transformers**, CVPR 2022.
  
  *Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe.*

  [[Paper](https://arxiv.org/abs/2204.08721)] [Code] ![](https://img.shields.io/badge/TokenFusion-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Fusion-orange)

- **[22] Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large Language Model**, arXiv 2024.
  
  *Liu, Ting and Shi, Liangtao and Hong, Richang and Hu, Yue and Yin, Quanjun and Zhang, Linfeng.*

  [[Paper](https://arxiv.org/abs/2411.10803)] [Code] ![](https://img.shields.io/badge/MultiStage_TokenDrop-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Stage_Based-purple) ![](https://img.shields.io/badge/Token_Dropping-orange)

- **[23] DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models**, arXiv 2024.
  
  *Tao, Keda and Qin, Can and You, Haoxuan and Sui, Yang and Wang, Huan.*

  [[Paper](https://arxiv.org/abs/2411.15024)] [Code] ![](https://img.shields.io/badge/DyCoke-blue) ![](https://img.shields.io/badge/Text_to_Image-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)


- **[24] AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning**, arXiv 2024.
  
  *Zhong, Yiwu and Liu, Zhuoming and Li, Yin and Wang, Liwei.*

  [[Paper](https://arxiv.org/abs/2412.03248)] [Code] ![](https://img.shields.io/badge/AIM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Adaptive_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[25] [CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster**, arXiv 2024.
  
  *Zhang, Qizhe and Cheng, Aosong and Lu, Ming and Zhuo, Zhiyong and Wang, Minqi and Cao, Jiajun and Guo, Shaobo and She, Qi and Zhang, Shanghang.*

  [[Paper](https://arxiv.org/abs/2412.01818)] [Code] ![](https://img.shields.io/badge/CLS_Attention-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[26] vid-TLDR: Training Free Token Merging for Light-weight Video Transformer**, CVPR 2024.
  
  *Choi, Joonmyung and Lee, Sanghyeok and Chu, Jaewon and Choi, Minhyuk and Kim, Hyunwoo J.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Choi_vid-TLDR_Training_Free_Token_Merging_for_Light-weight_Video_Transformer_CVPR_2024_paper.html)] [Code] ![](https://img.shields.io/badge/vid_TLDR-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[27] ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens**, arXiv 2024.
  
  *Guo, Yangyang and Zhang, Haoyu and Wong, Yongkang and Nie, Liqiang and Kankanhalli, Mohan.*

  [[Paper](https://arxiv.org/abs/2309.16738)] [Code] ![](https://img.shields.io/badge/ELIP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Reduction-orange)


- **[28] FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression**, arXiv 2024.
  
  *Zhu, Yuke and Xie, Chi and Liang, Shuang and Zheng, Bo and Guo, Sheng.*

  [[Paper](https://arxiv.org/abs/2411.14228)] [Code] ![](https://img.shields.io/badge/FocusLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)

- **[29] Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration**, arXiv 2024.
  
  *Han, Yuhang and Liu, Xuyang and Ding, Pengxiang and Wang, Donglin and Chen, Honggang and Yan, Qingsen and Huang, Siteng.*

  [[Paper](https://arxiv.org/abs/2411.17686)] [Code] ![](https://img.shields.io/badge/UnifiedTR-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Unified_Based-purple) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[30] Accelerating MLLMs by Searching Optimal Vision Token Reduction**, arXiv 2024.
  
  *Zhao, Shiyu and Wang, Zhenting and Juefei-Xu, Felix and Xia, Xide and Liu, Miao and Wang, Xiaofang and Liang, Mingfu and Zhang, Ning and Metaxas, Dimitris N and Yu, Licheng.*

  [[Paper](https://arxiv.org/abs/2412.00556)] [Code] ![](https://img.shields.io/badge/OptimalTR-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Search_Based-purple) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[31] Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See**, arXiv 2024.
  
  *Zhang, Zeliang and Pham, Phu and Zhao, Wentian and Wan, Kun and Li, Yu-Jhe and Zhou, Jianing and Miranda, Daniel and Kale, Ajinkya and Xu, Chenliang.*

  [[Paper](https://arxiv.org/abs/2410.06169)] [Code] ![](https://img.shields.io/badge/VisualAsText-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Text_Based-purple) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[32] An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models**, ECCV 2024.
  
  *Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-73004-7_2)] [Code] ![](https://img.shields.io/badge/HalfToken-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Layer_Based-purple) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[33] Is Less More? Exploring Token Condensation as Training-free Adaptation for CLIP**, arXiv 2024.
  
  *Wang, Zixin and Gong, Dong and Wang, Sen and Huang, Zi and Luo, Yadan.*

  [[Paper](https://arxiv.org/abs/2410.14729)] [Code] ![](https://img.shields.io/badge/TokenCond-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Condensation-orange)

- **[34] PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models**, arXiv 2024.
  
  *Wu, Tianxiang and Nie, Minxin and Cao, Ziqiang.*

  [[Paper](https://arxiv.org/abs/2410.07278)] [Code] ![](https://img.shields.io/badge/PAR-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Prompt_Based-purple) ![](https://img.shields.io/badge/Token_Reduction-orange)

- **[35] Inference Optimal VLMs Need Only One Visual Token but Larger Models**, arXiv 2024.
  
  *Li, Kevin Y and Goyal, Sachin and Semedo, Joao D and Kolter, J Zico.*

  [[Paper](https://arxiv.org/abs/2411.03312)] [Code] ![](https://img.shields.io/badge/OneToken-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Reduction-orange)


- **[36] HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models**, arXiv 2024.
  
  *Arif, Kazi Hasan Ibn and Yoon, JinYi and Nikolopoulos, Dimitrios S and Vandierendonck, Hans and John, Deepu and Ji, Bo.*

  [[Paper](https://arxiv.org/abs/2408.10945)] [Code] ![](https://img.shields.io/badge/HiRED-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Dropping-orange)

- **[37] Dynamic-VLM: Simple Dynamic Visual Token Compression for VideoLLM**, arXiv 2024.
  
  *Wang, Han and Nie, Yuxiang and Ye, Yongjie and GuanYu, Deng and Wang, Yanjie and Li, Shuai and Yu, Haiyang and Lu, Jinghui and Huang, Can.*

  [[Paper](https://arxiv.org/abs/2412.09530)] [Code] ![](https://img.shields.io/badge/DynamicVLM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)

- **[38] Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models**, ECCV 2024.
  
  *Ju, Chen and Wang, Haicheng and Li, Zeqian and Chen, Xu and Zhai, Zhonghua and Huang, Weilin and Xiao, Shuai.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-72952-2_25)] [Code] ![](https://img.shields.io/badge/Turbo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Informativity_Based-purple) ![](https://img.shields.io/badge/Token_Selection-orange)

- **[39] IVTP: Instruction-Guided Visual Token Pruning for Large Vision-Language Models**, ECCV 2024.
  
  *Huang, Kai and Zou, Hao and Xi, Ye and Wang, BoChen and Xie, Zhen and Yu, Liang.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-72643-9_13)] [Code] ![](https://img.shields.io/badge/IVTP-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Instruction_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[40] DiCoDe: Diffusion-Compressed Deep Tokens for Autoregressive Video Generation with Language Models**, arXiv 2024.
  
  *Li, Yizhuo and Ge, Yuying and Ge, Yixiao and Luo, Ping and Shan, Ying.*

  [[Paper](https://arxiv.org/abs/2412.04446)] [Code] ![](https://img.shields.io/badge/DiCoDe-blue) ![](https://img.shields.io/badge/Text_to_Image-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Compression-orange)

- **[41] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval**, arXiv 2024.
  
  *Shen, Leqi and Hao, Tianxiang and Zhao, Sicheng and Zhang, Yifeng and Liu, Pengzhang and Bao, Yongjun and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2409.01156)] [Code] ![](https://img.shields.io/badge/TempMe-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Temporal_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[42] Beyond Training: Dynamic Token Merging for Zero-Shot Video Understanding**, arXiv 2024.
  
  *Zhang, Yiming and Zhao, Zhuokai and Chen, Zhaorun and Ding, Zenghui and Yang, Xianjun and Sun, Yining.*

  [[Paper](https://arxiv.org/abs/2411.14401)] [Code] ![](https://img.shields.io/badge/DYTO-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[43] PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models**, arXiv 2024.

  *Yang, Chenyu and Dong, Xuan and Zhu, Xizhou and Su, Weijie and Wang, Jiahao and Tian, Hao and Chen, Zhe and Wang, Wenhai and Lu, Lewei and and Dai, Jifeng.*

  [[Paper](https://arxiv.org/abs/2412.09613)] [[Code](https://github.com/OpenGVLab/PVC)] ![](https://img.shields.io/badge/PVC-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)
  




