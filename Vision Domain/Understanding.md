Ê†πÊçÆÊèê‰æõÁöÑBibTeXÂºïÁî®ÔºåÂ∑≤Áªè‰∏∫ÊØèÁØáÊñáÁ´†ÁîüÊàê‰∫ÜÂØπÂ∫îÁöÑÊ†áÁ≠æ„ÄÇ‰ª•‰∏ãÊòØÊ†ºÂºèÂåñÂêéÁöÑÊñáÁ´†ÂàóË°®ÂíåÊ†áÁ≠æÔºö

---

<div align=center>

# Awesome Token-wise Generation Acceleration

üì¢ Collections of Awesome Token-wise Generation Acceleration Resources.

</div>

- **[1] Efficient Vision Transformer via Token Merger**, IEEE Transactions on Image Processing 2023.
  
  *Feng, Zhanzhou and Zhang, Shiliang.*
  
  [[Paper](https://ieeexplore.ieee.org/abstract/document/10183862)] ![](https://img.shields.io/badge/Token_Merging-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/ViT-orange)

- **[2] Token Merging: Your ViT But Faster**, arXiv 2022.
  
  *Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy.*
  
  [[Paper](https://arxiv.org/abs/2210.09461)] ![](https://img.shields.io/badge/Token_Merging-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/ViT-orange)

- **[3] Efficient Transformer Adaptation with Soft Token Merging**, CVPRW 2024.
  
  *Yuan, Xin and Fei, Hongliang and Baek, Jinoo.*
  
  [[Paper](https://openaccess.thecvf.com/content/CVPR2024W/ELVM/papers/Yuan_Efficient_Transformer_Adaptation_with_Soft_Token_Merging_CVPRW_2024_paper.pdf)] ![](https://img.shields.io/badge/Soft_Token_Merging-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/Adaptation-orange)

- **[4] Agglomerative Token Clustering**, ECCV 2025.
  
  *Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W and Moeslund, Thomas B.*
  
  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-72998-0_12)] ![](https://img.shields.io/badge/Token_Clustering-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/Clustering-orange)

- **[5] Token Pruning using a Lightweight Background Aware Vision Transformer**, arXiv 2024.
  
  *Sah, Sudhakar and Kumar, Ravish and Rohmetra, Honnesh and Saboori, Ehsan.*
  
  [[Paper](https://arxiv.org/abs/2410.09324)] ![](https://img.shields.io/badge/Token_Pruning-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/Background_Aware-orange)

- **[6] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification**, NeurIPS 2021.
  
  *Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui.*
  
  [[Paper](https://proceedings.neurips.cc/paper/2021/hash/747d3443e319a22747fbb873e8b2f9f2-Abstract.html)] ![](https://img.shields.io/badge/Dynamic_Token_Sparsification-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/ViT-orange)

- **[7] GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation**, WACV 2024.
  
  *Xu, Xuwei and Wang, Sen and Chen, Yudong and Zheng, Yanping and Wei, Zhewei and Liu, Jiajun.*
  
  [[Paper](https://openaccess.thecvf.com/content/WACV2024/html/Xu_GTP-ViT_Efficient_Vision_Transformers_via_Graph-Based_Token_Propagation_WACV_2024_paper.html)] ![](https://img.shields.io/badge/Graph_Token_Propagation-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/ViT-orange)



- **[8] PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference**, arXiv 2024.
  
  *Li, Ye and Tang, Chen and Meng, Yuan and Fan, Jiajun and Chai, Zenghao and Ma, Xinzhu and Wang, Zhi and Zhu, Wenwu.*
  
  [[Paper](https://arxiv.org/abs/2407.05010)] ![](https://img.shields.io/badge/PRANCE-blue) ![](https://img.shields.io/badge/Token_Optimization-orange) ![](https://img.shields.io/badge/Channel_Pruning-green) ![](https://img.shields.io/badge/ViT-orange)

- **[9] TPC-ViT: Token Propagation Controller for Efficient Vision Transformer**, arXiv 2024.
  
  *Zhu, Wentao.*
  
  [[Paper](https://arxiv.org/abs/2401.01470)] ![](https://img.shields.io/badge/TPC-ViT-blue) ![](https://img.shields.io/badge/Token_Propagation-orange) ![](https://img.shields.io/badge/ViT-green)

- **[10] Efficient Visual Transformer by Learnable Token Merging**, arXiv 2024.
  
  *Wang, Yancheng and Yang, Yingzhen.*
  
  [[Paper](https://arxiv.org/abs/2407.15219)] ![](https://img.shields.io/badge/Learnable_Token_Merging-blue) ![](https://img.shields.io/badge/Transformer-green) ![](https://img.shields.io/badge/ViT-orange)

- **[11] Unified Visual Transformer Compression**, arXiv 2022.
  
  *Yu, Shixing and Chen, Tianlong and Shen, Jiayi and Yuan, Huan and Tan, Jianchao and Yang, Sen and Liu, Ji and Wang, Zhangyang.*
  
  [[Paper](https://arxiv.org/abs/2203.08243)] ![](https://img.shields.io/badge/Unified_Compression-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Transformer-orange)

- **[12] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval**, arXiv 2024.
  
  *Shen, Leqi and Hao, Tianxiang and Zhao, Sicheng and Zhang, Yifeng and Liu, Pengzhang and Bao, Yongjun and Ding, Guiguang.*
  
  [[Paper](https://arxiv.org/abs/2409.01156)] ![](https://img.shields.io/badge/TempMe-blue) ![](https://img.shields.io/badge/Video_Retrieval-green) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Training-free-blue)

- **[13] HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers**, HPCA 2023.
  
  *Dong, Peiyan and Sun, Mengshu and Lu, Alec and Xie, Yanyue and Liu, Kenneth and Kong, Zhenglun and Meng, Xin and Li, Zhengang and Lin, Xue and Fang, Zhenman and others.*
  
  [[Paper](https://ieeexplore.ieee.org/abstract/document/10071047)] ![](https://img.shields.io/badge/HeatViT-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[14] Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers**, arXiv 2024.
  
  *Lee, Dong Hoon and Hong, Seunghoon.*
  
  [[Paper](https://arxiv.org/abs/2412.10569)] ![](https://img.shields.io/badge/Token_Merging-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Decoupled_Embedding-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[15] AdapTiV: Sign-Similarity Based Image-Adaptive Token Merging for Vision Transformer Acceleration**, MICRO 2024.
  
  *Yoo, Seungjae and Kim, Hangyeol and Kim, Joo-Young.*
  
  [[Paper](https://ieeexplore.ieee.org/abstract/document/10764572)] ![](https://img.shields.io/badge/AdapTiV-blue) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Image_Adaptive-green) ![](https://img.shields.io/badge/Training-based-blue)

- **[16] Energy Minimizing-based Token Merging for Accelerating Transformers**, Workshop 2024.
  
  *Tran, Hoai-Chau and Nguyen, Duy Minh Ho and Nguyen, Manh-Duy and Le, Ngan Hoang and Nguyen, Binh T.*
  
  [[Paper](https://openreview.net/forum?id=R7dCHc2Rp0)] ![](https://img.shields.io/badge/Energy_Token_Merging-blue) ![](https://img.shields.io/badge/Transformer-orange) ![](https://img.shields.io/badge/Training-free-blue)

- **[17] Zero-TPrune: Zero-shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers**, CVPR 2024.
  
  *Wang, Hongjie and Dedhia, Bhishma and Jha, Niraj K.*
  
  [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html)] ![](https://img.shields.io/badge/Zero-TPrune-blue) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Attention_Graph-green) ![](https://img.shields.io/badge/Training-free-blue)

- **[18] Shunted Self-Attention via Multi-Scale Token Aggregation**, CVPR 2022.
  
  *Ren, Sucheng and Zhou, Daquan and He, Shengfeng and Feng, Jiashi and Wang, Xinchao.*
  
  [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Ren_Shunted_Self-Attention_via_Multi-Scale_Token_Aggregation_CVPR_2022_paper.html)] ![](https://img.shields.io/badge/Shunted_Attention-blue) ![](https://img.shields.io/badge/Multi-Scale_Token_Aggregation-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[19] Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation**, ICCV 2023.
  
  *Tang, Quan and Zhang, Bowen and Liu, Jiajun and Liu, Fagui and Liu, Yifan.*
  
  [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Dynamic_Token_Pruning_in_Plain_Vision_Transformers_for_Semantic_Segmentation_ICCV_2023_paper.html)] ![](https://img.shields.io/badge/Token_Pruning-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Semantic_Segmentation-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[20] Token Turing Machines are Efficient Vision Models**, arXiv 2024.
  
  *Jajal, Purvish and Eliopoulos, Nick John and Chou, Benjamin Shiue-Hal and Thiravathukal, George K and Davis, James C and Lu, Yung-Hsiang.*
  
  [[Paper](https://arxiv.org/abs/2409.07613)] ![](https://img.shields.io/badge/Token_Turing_Machine-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Efficient_Model-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[21] Making Vision Transformers Efficient From a Token Sparsification View**, CVPR 2023.
  
  *Chang, Shuning and Wang, Pichao and Lin, Ming and Wang, Fan and Zhang, David Junhao and Jin, Rong and Shou, Mike Zheng.*
  
  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chang_Making_Vision_Transformers_Efficient_From_a_Token_Sparsification_View_CVPR_2023_paper.html)] ![](https://img.shields.io/badge/Token_Sparsification-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Efficient-Model-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[22] PSViT: Better Vision Transformer via Token Pooling and Attention Sharing**, arXiv 2021.
  
  *Chen, Boyu and Li, Peixia and Li, Baopu and Li, Chuming and Bai, Lei and Lin, Chen and Sun, Ming and Yan, Junjie and Ouyang, Wanli.*
  
  [[Paper](https://arxiv.org/abs/2108.03428)] ![](https://img.shields.io/badge/PSViT-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Token_Pooling-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[23] Content-Aware Token Sharing for Efficient Semantic Segmentation With Vision Transformers**, CVPR 2023.
  
  *Lu, Chenyang and de Geus, Daan and Dubbelman, Gijs.*
  
  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.html)] ![](https://img.shields.io/badge/Content-Aware_Token_Sharing-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Semantic_Segmentation-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[24] An Attention-Based Token Pruning Method for Vision Transformers**, IJCRS 2022.
  
  *Luo, Kaicheng and Li, Huaxiong and Zhou, Xianzhong and Huang, Bing.*
  
  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-21244-4_21)] ![](https://img.shields.io/badge/Attention-Based_Token_Pruning-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Pruning-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[25] LookupViT: Compressing Visual Information to a Limited Number of Tokens**, ECCV 2025.
  
  *Koner, Rajat and Jain, Gagan and Jain, Prateek and Tresp, Volker and Paul, Sujoy.*
  
  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-73016-0_19)] ![](https://img.shields.io/badge/LookupViT-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Token_Compression-orange) ![](https://img.shields.io/badge/Training-free-blue)

- **[26] Exploring Token Pruning in Vision State Space Models**, arXiv 2024.
  
  *Zhan, Zheng and Kong, Zhenglun and Gong, Yifan and Wu, Yushu and Meng, Zichong and Zheng, Hangyu and Shen, Xuan and Ioannidis, Stratis and Niu, Wei and Zhao, Pu and others.*
  
  [[Paper](https://arxiv.org/abs/2409.18962)] ![](https://img.shields.io/badge/Token_Pruning-blue) ![](https://img.shields.io/badge/State_Space_Models-orange) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Training-free-blue)

- **[27] M2M-TAG: Training-Free Many-to-Many Token Aggregation for Vision Transformer Acceleration**, NeurIPS 2024.
  
  *Zeng, Fanhu and Yu, Deli.*
  
  [[Paper](https://openreview.net/forum?id=LO3Mw8Jrk0)] ![](https://img.shields.io/badge/M2M-TAG-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Token_Aggregation-orange) ![](https://img.shields.io/badge/Training-free-blue)

- **[28] Accelerating Transformer-Based Scene Text Detection and Recognition via Token Pruning**, ICDAR 2023.
  
  *Garcia-Bordils, Sergi and Karatzas, Dimosthenis and Rusi√±ol, Mar√ßal.*
  
  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-41731-3_7)] ![](https://img.shields.io/badge/Token_Pruning-blue) ![](https://img.shields.io/badge/Scene_Text_Detection-orange) ![](https://img.shields.io/badge/Transformer-based-green) ![](https://img.shields.io/badge/Training-based-blue)

- **[29] AdaViT: Adaptive Tokens for Efficient Vision Transformer**, arXiv 2021.
  
  *Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo.*
  
  [[Paper](https://arxiv.org/abs/2112.07658)] ![](https://img.shields.io/badge/AdaViT-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Adaptive_Tokens-orange) ![](https://img.shields.io/badge/Training-based-blue)

- **[30] DOTA: Detect and Omit Weak Attentions for Scalable Transformer Acceleration**, ASPLOS 2022.
  
  *Qu, Zheng and Liu, Liu and Tu, Fengbin and Chen, Zhaodong and Ding, Yufei and Xie, Yuan.*
  
  [[Paper](https://dl.acm.org/doi/abs/10.1145/3503222.3507738)] ![](https://img.shields.io/badge/DOTA-blue) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Weak_Attentions-green) ![](https://img.shields.io/badge/Training-free-blue)

- **[31] Adaptive Sparse ViT: Towards Learnable Adaptive Token Pruning by Fully Exploiting Self-Attention**, arXiv 2022.
  
  *Liu, Xiangcheng and Wu, Tianyi and Guo, Guodong.*
  
  [[Paper](https://arxiv.org/abs/2209.13802)] ![](https://img.shields.io/badge/Adaptive_Sparse_ViT-blue) ![](https://img.shields.io/badge/ViT-green) ![](https://img.shields.io/badge/Self-Attention-orange) ![](https://img.shields.io/badge)

  
