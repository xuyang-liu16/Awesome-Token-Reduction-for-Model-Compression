### üó∫Ô∏è Image Recognition
- **[1] Adaptive Token Sampling for Efficient Vision Transformers**, ECCV 2022.
  
  *Fayyaz, Mohsen and Koohpayegani, Soroush Abbasi and Jafari, Farnoush Rezaei and Sengupta, Sunando and Joze, Hamid Reza Vaezi and Sommerlade, Eric and Pirsiavash, Hamed and Gall, J√ºrgen.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_24)] [[Code]( https://adaptivetokensampling.github.io/)] ![](https://img.shields.io/badge/ATS-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Sampling-orange)

- **[2] SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning**, ECCV 2022.
  
  *Kong, Zhenglun and Dong, Peiyan and Ma, Xiaolong and Meng, Xin and Niu, Wei and Sun, Mengshu and Shen, Xuan and Yuan, Geng and Ren, Bin and Tang, Hao and others.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_37)] [[Code]( https://github.com/PeiyanFlying/SPViT)] ![](https://img.shields.io/badge/SPViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Latency_Aware-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[3] SaiT: Sparse Vision Transformers through Adaptive Token Pruning**, arXiv 2022.
  
  *Li, Ling and Thorsley, David and Hassoun, Joseph.*

  [[Paper](https://arxiv.org/abs/2210.05832)] [Code] ![](https://img.shields.io/badge/SaiT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] Not all patches are what you need: Expediting vision transformers via token reorganizations**, arXiv 2022.
  
  *Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao.*

  [[Paper](https://arxiv.org/abs/2202.07800)] [[Code]([https://github.com/youweiliao/TokenReorganization](https://github.com/youweiliang/evit))] ![](https://img.shields.io/badge/NAP-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Reorganization-orange)

- **[5] Beyond attentive tokens: Incorporating token importance and diversity for efficient vision transformers**, CVPR 2023.
  
  *Long, Sifan and Zhao, Zhen and Pi, Jimin and Wang, Shengsheng and Wang, Jingdong.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Long_Beyond_Attentive_Tokens_Incorporating_Token_Importance_and_Diversity_for_Efficient_CVPR_2023_paper.html)] [Code] ![](https://img.shields.io/badge/BAT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Importance_Diversity-purple)

- **[6] IA-RED¬≤: Interpretability-aware Redundancy Reduction for Vision Transformers**, NeurIPS 2021.
  
  *Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude.*

  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2021/hash/d072677d210ac4c03ba046120f0802ec-Abstract.html)] [[Code]([https://github.com/bowenpan/IA-RED2](http://people.csail.mit.edu/bpan/ia-red/))] ![](https://img.shields.io/badge/IA_RED2-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Redundancy_Reduction-purple)

- **[7] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification**, NeurIPS 2021.
  
  *Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui.*

  [[Paper](https://proceedings.neurips.cc/paper/2021/hash/747d3443e319a22747fbb873e8b2f9f2-Abstract.html)] [[Code](https://github.com/RaoYongming/DynamicViT)]] ![](https://img.shields.io/badge/DynamicViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Sparsification-orange)

- **[8] Patch Slimming for Efficient Vision Transformers**, CVPR 2022.
  
  *Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Tang_Patch_Slimming_for_Efficient_Vision_Transformers_CVPR_2022_paper.html)] [Code] ![](https://img.shields.io/badge/PatchSlimming-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Patch_Slimming-purple)

- **[9] Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer**, AAAI 2022.
  
  *Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing.*

  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/20202)] [Code] ![](https://img.shields.io/badge/EvoViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Evolution-orange)

- **[10] A-ViT: Adaptive Tokens for Efficient Vision Transformer**, CVPR 2022.
  
  *Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Yin_A-ViT_Adaptive_Tokens_for_Efficient_Vision_Transformer_CVPR_2022_paper.html)] [[Code]([https://github.com/hongxuYin/A-ViT](https://a-vit.github.io/))] ![](https://img.shields.io/badge/A-ViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Adaptive_Tokens-purple)

- **[11] Multi-scale Hybrid Vision Transformer and Sinkhorn Tokenizer for Sewer Defect Classification**, Automation in Construction 2022.
  
  *Haurum, Joakim Bruslund and Madadi, Meysam and Escalera, Sergio and Moeslund, Thomas B.*

  [[Paper](https://www.sciencedirect.com/science/article/pii/S0926580522004848)] [Code] ![](https://img.shields.io/badge/MultiScaleViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Sinkhorn_Tokenizer-purple)

- **[12] Token Pooling in Vision Transformers for Image Classification**, WACV 2023.
  
  *Marin, Dmitrii and Chang, Jen-Hao Rick and Ranjan, Anurag and Prabhu, Anish and Rastegari, Mohammad and Tuzel, Oncel.*

  [[Paper](https://openaccess.thecvf.com/content/WACV2023/html/Marin_Token_Pooling_in_Vision_Transformers_for_Image_Classification_WACV_2023_paper.html)] [Code] ![](https://img.shields.io/badge/TokenPooling-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Pooling-purple)

- **[13] Learning to Merge Tokens in Vision Transformers**, arXiv 2022.
  
  *Renggli, Cedric and Pinto, Andr√© Susano and Houlsby, Neil and Mustafa, Basil and Puigcerver, Joan and Riquelme, Carlos.*

  [[Paper](https://arxiv.org/abs/2202.12015)] [Code] ![](https://img.shields.io/badge/LearnTokenMerging-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Merging-purple)

- **[14] Centroid Transformers: Learning to Abstract with Attention**, arXiv 2021.
  
  *Wu, Lemeng and Liu, Xingchao and Liu, Qiang.*

  [[Paper](https://arxiv.org/abs/2102.08606)] [Code] ![](https://img.shields.io/badge/CentroidTransformers-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple)

- **[15] GroupViT: Semantic Segmentation Emerges From Text Supervision**, CVPR 2022.
  
  *Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Xu_GroupViT_Semantic_Segmentation_Emerges_From_Text_Supervision_CVPR_2022_paper.html)] [[Code]([https://github.com/JiaruiXu/GroupViT](https://github.com/NVlabs/GroupViT))] ![](https://img.shields.io/badge/GroupViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Text_Supervision-purple)

- **[16] Not all tokens are equal: Human-centric visual analysis via token clustering transformer**, CVPR 2022.
  
  *Zeng, Wang and Jin, Sheng and Liu, Wentao and Qian, Chen and Luo, Ping and Ouyang, Wanli and Wang, Xiaogang.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zeng_Not_All_Tokens_Are_Equal_Human-Centric_Visual_Analysis_via_Token_Clustering_Transformer_CVPR_2022_paper.html)] [[Code](https://github.com/wangzeng/token-clustering-transformer)] ![](https://img.shields.io/badge/TokenClusteringTransformer-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Clustering-purple)

- **[17] Self-slimmed Vision Transformer**, ECCV 2022.
  
  *Zong, Zhuofan and Li, Kunchang and Song, Guanglu and Wang, Yali and Qiao, Yu and Leng, Biao and Liu, Yu.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-20083-0_26)] [[Code](https://github.com/Sense-X/SiT)] ![](https://img.shields.io/badge/SelfSlimmedViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Self_Slimming-purple)

- **[18] Which Tokens to Use? Investigating Token Reduction in Vision Transformers**, ICCVw 2023.
  
  *Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W and Moeslund, Thomas B.*

  [[Paper](https://openaccess.thecvf.com/content/ICCV2023W/NIVT/html/Haurum_Which_Tokens_to_Use_Investigating_Token_Reduction_in_Vision_Transformers_ICCVW_2023_paper.html)] [[Code](https://vap.aau.dk/tokens)] ![](https://img.shields.io/badge/TokenReduction-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) 

- **[19] Efficient Vision Transformer via Token Merger**, TIP 2023.
  
  *Feng, Zhanzhou and Zhang, Shiliang.*

  [[Paper](https://ieeexplore.ieee.org/abstract/document/10183862)] [Code] ![](https://img.shields.io/badge/TokenMerger-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[20] Token Merging: Your ViT But Faster**, ICLR 2023.
  
  *Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy.*

  [[Paper](https://arxiv.org/abs/2210.09461)] [[Code]([https://github.com/facebookresearch/ToMe](https://github.com/facebookresearch/ToMe))] ![](https://img.shields.io/badge/ToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[21] Efficient Transformer Adaptation with Soft Token Merging**, CVPRW 2024.
  
  *Yuan, Xin and Fei, Hongliang and Baek, Jinoo.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024W/ELVM/papers/Yuan_Efficient_Transformer_Adaptation_with_Soft_Token_Merging_CVPRW_2024_paper.pdf)] [Code] ![](https://img.shields.io/badge/SoftToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Machine_Translation-green) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[22] Agglomerative Token Clustering**, ECCV 2024.
  
  *Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W and Moeslund, Thomas B.*

  [[Paper](https://arxiv.org/abs/2409.11923)] [[Code](https://github.com/JoakimHaurum/ATC)] ![](https://img.shields.io/badge/ATC-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Image_Generation-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[23] Token Pruning using a Lightweight Background Aware Vision Transformer**, arXiv 2024.
  
  *Sah, Sudhakar and Kumar, Ravish and Rohmetra, Honnesh and Saboori, Ehsan.*

  [[Paper](https://arxiv.org/abs/2410.09324)] [Code] ![](https://img.shields.io/badge/BAViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[24] GTP-ViT: Efficient Vision Transformers via Graph-Based Token Propagation**, WACV 2024.
  
  *Xu, Xuwei and Wang, Sen and Chen, Yudong and Zheng, Yanping and Wei, Zhewei and Liu, Jiajun.*

  [[Paper](https://arxiv.org/abs/2311.03035)] [[Code](https://github.com/Ackesnal/GTP-ViT)] ![](https://img.shields.io/badge/GTP_ViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[25] PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference**, arXiv 2024.
  
  *Li, Ye and Tang, Chen and Meng, Yuan and Fan, Jiajun and Chai, Zenghao and Ma, Xinzhu and Wang, Zhi and Zhu, Wenwu.*

  [[Paper](https://arxiv.org/abs/2407.05010)] [Code] ![](https://img.shields.io/badge/PRANCE-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Channel_Pruning-orange)

- **[26] TPC-ViT: Token Propagation Controller for Efficient Vision Transformer**, WACV 2024.
  
  *Zhu, Wentao.*

  [[Paper](https://arxiv.org/abs/2401.01470)] [Code] ![](https://img.shields.io/badge/TPC_ViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Caching-orange)

- **[27] Efficient Visual Transformer by Learnable Token Merging**, arXiv 2024.
  
  *Wang, Yancheng and Yang, Yingzhen.*

  [[Paper](https://arxiv.org/abs/2407.15219)] [[Code](https://github.com/Statistical-Deep-Learning/LTM)] ![](https://img.shields.io/badge/LToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[28] Unified Visual Transformer Compression**, ICLR 2022.
  
  *Yu, Shixing and Chen, Tianlong and Shen, Jiayi and Yuan, Huan and Tan, Jianchao and Yang, Sen and Liu, Ji and Wang, Zhangyang.*

  [[Paper](https://arxiv.org/abs/2203.08243)] [[Code](https://github.com/VITA-Group/UVC)] ![](https://img.shields.io/badge/UVC-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Layer_Pruning-orange) ![](https://img.shields.io/badge/Knowledge_Distillation-orange)

- **[29] HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers**, HPCA 2023.
  
  *Dong, Peiyan and Sun, Mengshu and Lu, Alec and Xie, Yanyue and Liu, Kenneth and Kong, Zhenglun and Meng, Xin and Li, Zhengang and Lin, Xue and Fang, Zhenman and others.*

  [[Paper](https://ieeexplore.ieee.org/abstract/document/10071047)] [Code] ![](https://img.shields.io/badge/HeatViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[30] Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers**, arXiv 2024.
  
  *Lee, Dong Hoon and Hong, Seunghoon.*

  [[Paper](https://arxiv.org/abs/2412.10569)] [Code] ![](https://img.shields.io/badge/DEToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[31] AdapTiV: Sign-Similarity Based Image-Adaptive Token Merging for Vision Transformer Acceleration**, MICRO 2024.
  
  *Yoo, Seungjae and Kim, Hangyeol and Kim, Joo-Young.*

  [[Paper](https://ieeexplore.ieee.org/abstract/document/10764572)] [Code] ![](https://img.shields.io/badge/AdapTiV-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Sign_Similarity-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[32] Energy Minimizing-based Token Merging for Accelerating Transformers**, Workshop on practical ML 2024.
  
  *Tran, Hoai-Chau and Nguyen, Duy Minh Ho and Nguyen, Manh-Duy and Le, Ngan Hoang and Nguyen, Binh T.*

  [[Paper](https://openreview.net/forum?id=R7dCHc2Rp0)] [Code] ![](https://img.shields.io/badge/EMToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Energy_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[33] Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers**, CVPR 2024.
  
  *Wang, Hongjie and Dedhia, Bhishma and Jha, Niraj K.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_Zero-TPrune_Zero-Shot_Token_Pruning_through_Leveraging_of_the_Attention_Graph_CVPR_2024_paper.html)] [Code] ![](https://img.shields.io/badge/Zero_TPrune-blue) ![](https://img.shields.io/badge/Vision_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[34] PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference**, ECCV 2024.
  
  *Tanvir Mahmud, Burhaneddin Yaman, Chun-Hao Liu, Diana Marculescu.*

  [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-73337-6_7)] [Code] ![](https://img.shields.io/badge/PaPr-blue) ![](https://img.shields.io/badge/Vision_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Patch_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[35] Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers**, arXiv 2024.
  
  *Lee, Dong Hoon and Hong, Seunghoon.*

  [[Paper](https://arxiv.org/abs/2412.10569)] [Code] ![](https://img.shields.io/badge/DecoupledToMe-blue) ![](https://img.shields.io/badge/Vision_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[36] Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer**, arXiv 2024.
  
  *Shuai Peng, Di Fu, Baole Wei, Yong Cao, Liangcai Gao, Zhi Tang.*

  [[Paper](https://arxiv.org/abs/2408.17062)] [Code] ![](https://img.shields.io/badge/Vote&Mix-blue) ![](https://img.shields.io/badge/Vision_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Voting_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)
  

### üìπ Video Recognition:

- **[1] TokenLearner: Adaptive Space-Time Tokenization for Videos**, NeurIPS 2021.
  
  *Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, Anelia Angelova.*

  [[Paper](https://proceedings.neurips.cc/paper/2021/hash/6a30e32e56fce5cf381895dfe6ca7b6f-Abstract.html)] [[Code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner)] ![](https://img.shields.io/badge/TokenLearner-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[2] Efficient Video Transformers with Spatial-Temporal Token Selection**, ECCV 2022.
  
  *Junke Wang and Xitong Yang and Hengduo Li and Li Liu and Zuxuan Wu and Yu-Gang Jiang.*

  [[Paper](https://arxiv.org/abs/2111.11591)] [[Code](https://github.com/wdrink/STTS)] ![](https://img.shields.io/badge/STTS-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[3] Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation**, ICCV 2023.
  
  *Shuangrui Ding, Peisen Zhao, Xiaopeng Zhang, Rui Qian, Hongkai Xiong, Qi Tian.*

  [[Paper](https://arxiv.org/abs/2308.04549)] [Code] ![](https://img.shields.io/badge/PSTA-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Semantic_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] vid-TLDR: Training Free Token merging for Light-weight Video Transformer**, CVPR 2024.
  
  *Joonmyung Choi and Sanghyeok Lee and Jaewon Chu and Minhyuk Choi and Hyunwoo J. Kim.*

  [[Paper](https://arxiv.org/abs/2403.13347)] [[Code](https://github.com/mlvlab/vid-TLDR)] ![](https://img.shields.io/badge/vid_TLDR-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Salience_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[5] Efficient Video Transformers via Spatial-temporal Token Merging for Action Recognition**, ACM MM 2024.
  
  *Zhanzhou Feng, Jiaming Xu, Lei Ma, Shiliang Zhang.*

  [[Paper](https://dl.acm.org/doi/abs/10.1145/3633781)] [Code] ![](https://img.shields.io/badge/STToMe-blue) ![](https://img.shields.io/badge/Action_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Spatio_Temporal-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[6] Don't Look Twice: Faster Video Transformers with Run-Length Tokenization**, NeurIPS 2024.

  *Choudhury, Rohan and Zhu, Guanglei and Liu, Sihan and Niinuma, Koichiro and Kitani, Kris M and Jeni, Laszlo Attila.*

  [[Paper](https://dl.acm.org/doi/abs/10.1145/3633781)] [[Code](https://github.com/rccchoudhury/rlt)] ![](https://img.shields.io/badge/RLT-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Temporal_Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)
  

- **[7] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval**, arXiv 2024.
  
  *Shen, Leqi and Hao, Tianxiang and Zhao, Sicheng and Zhang, Yifeng and Liu, Pengzhang and Bao, Yongjun and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2409.01156)] [Code] ![](https://img.shields.io/badge/TempMe-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Text_Video_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[8] Efficient Video Action Detection with Token Dropout and Context Refinement**, ICCV 2023.
  
  *Chen, Lei and Tong, Zhan and Song, Yibing and Wu, Gangshan and Wang, Limin.*

  [[Paper](https://arxiv.org/abs/2304.08451)] [[Code](https://github.com/MCG-NJU/EVAD)] ![](https://img.shields.io/badge/EVAD-blue) ![](https://img.shields.io/badge/Video_Action_Detection-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)



### üîç Dense Prediction:

- **[1] Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer**, CVPR 2022.
  
  *Zeng, Wang and Jin, Sheng and Liu, Wentao and Qian, Chen and Luo, Ping and Ouyang, Wanli and Wang, Xiaogang.*

  [[Paper](https://arxiv.org/abs/2204.08680)] [[Code](https://github.com/zengwang430521/TCFormer)] ![](https://img.shields.io/badge/TCFormer-blue) ![](https://img.shields.io/badge/Pose_Estimation-green) ![](https://img.shields.io/badge/Mesh_Reconstruction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[2] Less is more: Pay less attention in vision transformers**, AAAI 2022.
  
  *Zizheng Pan and Bohan Zhuang and Haoyu He and Jing Liu and Jianfei Cai.*

  [[Paper](https://arxiv.org/abs/2105.14217)] [[Code](https://github.com/ziplab/LIT)] ![](https://img.shields.io/badge/LIT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Object_Detection-green) ![](https://img.shields.io/badge/Instance_Segmentation-green) ![](https://img.shields.io/badge/ISemantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[3] Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation**, ICCV 2023.
  
  *Tang, Quan and Zhang, Bowen and Liu, Jiajun and Liu, Fagui and Liu, Yifan.*

  [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Dynamic_Token_Pruning_in_Plain_Vision_Transformers_for_Semantic_Segmentation_ICCV_2023_paper.html)] [Code] ![](https://img.shields.io/badge/DTP-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] Content-Aware Token Sharing for Efficient Semantic Segmentation With Vision Transformers**, CVPR 2023.
  
  *Lu, Chenyang and de Geus, Daan and Dubbelman, Gijs.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.html)] [Code] ![](https://img.shields.io/badge/CATS-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Sharing-orange)

- **[5] Revisiting token pruning for object detection and instance segmentation**, WACV 2024.

  *Liu, Yifei and Gehrig, Mathias and Messikommer, Nico and Cannici, Marco and Scaramuzza, Davide.*
  
  [[Paper](https://arxiv.org/abs/2306.07050)] [[Code](https://github.com/uzh-rpg/svit)] ![](https://img.shields.io/badge/SViT-blue) ![](https://img.shields.io/badge/Object_Detection-green) ![](https://img.shields.io/badge/Instance_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[6] Dynamic Token-Pass Transformers for Semantic Segmentation**, WACV 2024.

  *Liu, Yuang and Zhou, Qiang and Wang, Jin and Wang, Zhibin and Wang, Fan and Wang, Jun and Zhang, Wei.*
  
  [[Paper](https://arxiv.org/abs/2308.01944)] [[Code](https://github.com/FLHonker/DoViT-code)] ![](https://img.shields.io/badge/DoViT-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[7] DTMFormer: Dynamic Token Merging for Boosting Transformer-Based Medical Image Segmentation**, AAAI 2024.

  *Wang, Zhehao and Lin, Xian and Wu, Nannan and Yu, Li and Cheng, Kwang-Ting and Yan, Zengqiang.*
  
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28394)] [[Code](https://github.com/iam-nacl/DTMFormer)] ![](https://img.shields.io/badge/DTMFormer-blue) ![](https://img.shields.io/badge/Medical_Image_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[8] Segformer++: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation**, MIPR 2024.
  
  *Daniel Kienzle and Marco Kantonis and Robin Sch√∂n and Rainer Lienhart.*
  
  [[Paper](https://arxiv.org/abs/2405.14467)] [Code] ![](https://img.shields.io/badge/Segformer++-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[9] ToSA: Token Selective Attention for Efficient Vision Transformers**, arXiv 2024.
  *Singh, Manish Kumar et al.*
  
  [[Paper](https://arxiv.org/abs/2406.08816)] [Code] ![](https://img.shields.io/badge/ToSA-blue) ![](https://img.shields.io/badge/Vision_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[10] Less is More: Token Context-aware Learning for Object Tracking**, AAAI 2025.
  *Chenlong Xu and Bineng Zhong and Qihua Liang and Yaozong Zheng and Guorong Li and Shuxiang Song.*
  
  [[Paper](https://arxiv.org/abs/2501.00758)] [[Code](https://github.com/XuChenLong/LMTrack)] ![](https://img.shields.io/badge/LMTrack-blue) ![](https://img.shields.io/badge/Object_Tracking-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[11] Efficient Video Action Detection with Token Dropout and Context Refinement**, ICCV 2023.
  
  *Chen, Lei and Tong, Zhan and Song, Yibing and Wu, Gangshan and Wang, Limin.*

  [[Paper](https://arxiv.org/abs/2304.08451)] [[Code](https://github.com/MCG-NJU/EVAD)] ![](https://img.shields.io/badge/EVAD-blue) ![](https://img.shields.io/badge/Video_Action_Detection-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  



