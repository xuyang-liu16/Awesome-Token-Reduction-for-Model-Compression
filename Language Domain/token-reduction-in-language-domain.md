# Token Reduction Techniques in Language Domain

Maintainers: [Junjie Chen](https://www.junjie-chen.info), [Xuyang Liu](https://xuyang-liu16.github.io/)

If your work is missing, please let us know by creating an issue or submitting a pull request.

Color code: ![Abbreviation](https://img.shields.io/badge/Abbreviation-blue) ![Application](https://img.shields.io/badge/Application-green) ![Reduction_Criteria](https://img.shields.io/badge/Reduction_Criteria-purple) ![Reduction_Mechanism](https://img.shields.io/badge/Reduction_Mechanism-orange) ![W./W.o._Training](https://img.shields.io/badge/W./W.o._Training-pink) ![Keywords](https://img.shields.io/badge/Keywords-yellow) 

## 1999

- **[1] What is linguistic redundancy**, University of Chicago 1999  
  *Wit, EC and Gillette, Marie*  
  [[Paper](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=182d3980d8f41616da690d607f6f980dbf582352)] ![Theory](https://img.shields.io/badge/Theory-yellow) 

## 2016

- **[1] Phased lstm: Accelerating recurrent network training for long or event-based sequences**, NeurIPS 2016  
  *Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii*  
  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2016/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf)] ![Phased_lstm](https://img.shields.io/badge/Phased_lstm-blue) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[2] Hierarchical question answering for long documents**, arXiv 2016  
  *Choi, Eunsol and Hewlett, Daniel and Lacoste, Alexandre and Polosukhin, Illia and Uszkoreit, Jakob and Berant, Jonathan*  
  [[Paper](https://arxiv.org/pdf/1611.01839)] ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow) 

## 2017

- **[1] Reasonet: Learning to stop reading in machine comprehension**, KDD 2017  
  *Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu*  
  [[Paper](https://arxiv.org/pdf/1609.05284)] ![Reasonet](https://img.shields.io/badge/Reasonet-blue) ![Marchine_Reading_Comprehension](https://img.shields.io/badge/Marchine_Reading_Comprehension-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow) 

- **[2] Learning to skim text**, ACL 2017  
  *Yu, Adams Wei and Lee, Hongrae and Le, Quoc V*  
  [[Paper](https://arxiv.org/pdf/1704.06877)] ![LSTM--Jump](https://img.shields.io/badge/LSTM--Jump-blue) ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow) 

- **[3] Learning when to skim and when to read**, RepL4NLP 2017  
  *Johansen, Alexander Rosenberg and Socher, Richard*  
  [[Paper](https://arxiv.org/pdf/1712.05483)] ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) 

- **[4] Training a subsampling mechanism in expectation**, ICLR 2017  
  *Raffel, Colin and Lawson, Dieterich*  
  [[Paper](https://arxiv.org/pdf/1702.06914)] ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) 

- **[5] Variable computation in recurrent neural networks**, ICLR 2017  
  *Jernite, Yacine and Grave, Edouard and Joulin, Armand and Mikolov, Tomas*  
  [[Paper](https://openreview.net/pdf?id=S1LVSrcge)] ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) 

## 2018

- **[1] Neural speed reading via skim-rnn**, ICLR 2018  
  *Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh*  
  [[Paper](https://openreview.net/pdf?id=Sy-dQG-Rb)] ![Skim--RNN_](https://img.shields.io/badge/Skim--RNN_-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) 

- **[2] Skip rnn: Learning to skip state updates in recurrent neural networks**, ICLR 2018  
  *Campos, Víctor and Jou, Brendan and Giró-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu*  
  [[Paper](https://openreview.net/pdf?id=HkwVAXyCW)] [[Code](https://github.com/imatge-upc/skiprnn-2017-telecombcn)] ![Skip_RNN_](https://img.shields.io/badge/Skip_RNN_-blue) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) 

- **[3] Accelerating neural transformer via an average attention network**, ACL 2018  
  *Zhang, Biao and Xiong, Deyi and Su, Jinsong*  
  [[Paper](https://aclanthology.org/P18-1166.pdf)] [[Code](https://github.com/bzhangGo/transformer-aan)] ![transformer--aan](https://img.shields.io/badge/transformer--aan-blue) ![Translation](https://img.shields.io/badge/Translation-green) ![Average_Attention](https://img.shields.io/badge/Average_Attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[4] Generating wikipedia by summarizing long sequences**, ICLR 2018  
  *Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam*  
  [[Paper](https://openreview.net/pdf?id=Hyg0vbWC-)] [[Code](https://github.com/lucidrains/memory-compressed-attention)] ![Summarization](https://img.shields.io/badge/Summarization-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

## 2019

- **[1] Transformer-XL: Attentive language models beyond a fixed-length context**, ACL 2019  
  *Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan*  
  [[Paper](https://aclanthology.org/P19-1285.pdf)] [[Code](https://github.com/kimiyoung/transformer-xl)] ![Transformer--XL](https://img.shields.io/badge/Transformer--XL-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Cache](https://img.shields.io/badge/Cache-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Memory](https://img.shields.io/badge/Memory-yellow) ![Recurrent_Mechanism](https://img.shields.io/badge/Recurrent_Mechanism-yellow) 

- **[2] Neural speed reading with structural-jump-lstm**, ICLR 2019  
  *Christian Hansen and Casper Hansen and Stephen Alstrup and Jakob Grue Simonsen and Christina Lioma*  
  [[Paper](https://openreview.net/pdf?id=B1xf9jAqFQ)] [[Code](https://github.com/Varyn/Neural-Speed-Reading-with-Structural-Jump-LSTM)] ![Structural--Jump--LSTM](https://img.shields.io/badge/Structural--Jump--LSTM-blue) ![Agent](https://img.shields.io/badge/Agent-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![RNN](https://img.shields.io/badge/RNN-yellow) 

- **[3] How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings**, EMNLP-IJCNLP 2019  
  *Ethayarajh, Kawin*  
  [[Paper](https://aclanthology.org/D19-1006/)] ![Explanation](https://img.shields.io/badge/Explanation-yellow) 

## 2020

- **[1] Compressive transformers for long-range sequence modelling**, ICLR 2020  
  *Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P*  
  [[Paper](https://openreview.net/pdf?id=SylKikSYDH)] [[Code](https://github.com/lucidrains/compressive-transformer-pytorch)] ![Compressive_Transformer](https://img.shields.io/badge/Compressive_Transformer-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Cache](https://img.shields.io/badge/Cache-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Most--used](https://img.shields.io/badge/Most--used-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Memory](https://img.shields.io/badge/Memory-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow) 

- **[2] PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination**, ICML 2020  
  *Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish*  
  [[Paper](https://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf)] [[Code](https://github.com/IBM/PoWER-BERT)] ![PoWER--BERT](https://img.shields.io/badge/PoWER--BERT-blue) ![QA](https://img.shields.io/badge/QA-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Embedding](https://img.shields.io/badge/Embedding-green) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![Static](https://img.shields.io/badge/Static-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[3] Funnel-transformer: Filtering out sequential redundancy for efficient language processing**, NeurIPS 2020  
  *Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc*  
  [[Paper](https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf)] [[Code](https://github.com/laiguokun/Funnel-Transformer)] ![Funnel--transformer](https://img.shields.io/badge/Funnel--transformer-blue) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow) 

- **[4] Multi-scale Transformer Language Models**, arXiv 2020  
  *Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y-Lan*  
  [[Paper](https://arxiv.org/pdf/2005.00581)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Memory](https://img.shields.io/badge/Memory-yellow) 

- **[5] Reformer: The efficient transformer**, ICLR 2020  
  *Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya*  
  [[Paper](https://openreview.net/pdf?id=rkgNKkHtvB)] [[Code](https://github.com/google/trax/tree/master/trax/models/reformer)] ![Reformer](https://img.shields.io/badge/Reformer-blue) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Translation](https://img.shields.io/badge/Translation-green) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Locality--Sensitive_Hashing](https://img.shields.io/badge/Locality--Sensitive_Hashing-orange) ![Cache](https://img.shields.io/badge/Cache-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow) 

- **[6] Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words**, ACL 2020  
  *Klafka, Josef and Ettinger, Allyson*  
  [[Paper](https://aclanthology.org/2020.acl-main.434.pdf)] [[Code](https://github.com/jklafka/context-probes)] ![Explanation](https://img.shields.io/badge/Explanation-yellow) 

- **[7] DeFormer: Decomposing pre-trained transformers for faster question answering**, ACL 2020  
  *Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan*  
  [[Paper](https://aclanthology.org/2020.acl-main.411.pdf)] [[Code](https://github.com/StonyBrookNLP/deformer)] ![DeFormer](https://img.shields.io/badge/DeFormer-blue) ![QA](https://img.shields.io/badge/QA-green) ![Sentence--Sentence](https://img.shields.io/badge/Sentence--Sentence-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Transformer_Encoder](https://img.shields.io/badge/Transformer_Encoder-yellow) 

- **[8] Attention is not only a weight: Analyzing transformers with vector norms**, EMNLP 2020  
  *Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro*  
  [[Paper](https://aclanthology.org/2020.emnlp-main.574.pdf)] [[Code](https://github.com/gorokoba560/norm-analysis-of-transformer)] ![Norm--based_Analysis](https://img.shields.io/badge/Norm--based_Analysis-yellow) ![Explanation](https://img.shields.io/badge/Explanation-yellow) 

## 2021

- **[1] TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference**, NAACL 2021  
  *Ye, Deming and Lin, Yankai and Huang, Yufei and Sun, Maosong*  
  [[Paper](https://aclanthology.org/2021.naacl-main.463.pdf)] [[Code](https://github.com/thunlp/TR-BERT)] ![TR--BERT](https://img.shields.io/badge/TR--BERT-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Random](https://img.shields.io/badge/Random-purple) ![Prediction--guided](https://img.shields.io/badge/Prediction--guided-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[2] Poolingformer: Long document modeling with pooling attention**, ICML  2021  
  *Zhang, Hang and Gong, Yeyun and Shen, Yelong and Li, Weisheng and Lv, Jiancheng and Duan, Nan and Chen, Weizhu*  
  [[Paper](http://proceedings.mlr.press/v139/zhang21h/zhang21h.pdf)] ![Poolingformer](https://img.shields.io/badge/Poolingformer-blue) ![Long_Sequence_Summarization](https://img.shields.io/badge/Long_Sequence_Summarization-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[3] Length-adaptive transformer: Train once with length drop, use anytime with search**, ACL 2021  
  *Kim, Gyuwan and Cho, Kyunghyun*  
  [[Paper](https://aclanthology.org/2021.acl-long.508.pdf)] [[Code](https://github.com/clovaai/length-adaptive-transformer)] ![Length--Adaptive_Transformer](https://img.shields.io/badge/Length--Adaptive_Transformer-blue) ![Sequence--leval_Classification](https://img.shields.io/badge/Sequence--leval_Classification-green) ![Token--leval_Classification](https://img.shields.io/badge/Token--leval_Classification-green) ![Drop--and--Restore](https://img.shields.io/badge/Drop--and--Restore-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Distillation](https://img.shields.io/badge/Distillation-yellow) ![LayerDrop](https://img.shields.io/badge/LayerDrop-yellow) ![LengthDrop](https://img.shields.io/badge/LengthDrop-yellow) 

- **[4] Efficient content-based sparse attention with routing transformers**, TACL 2021  
  *Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David*  
  [[Paper](https://aclanthology.org/2021.tacl-1.4.pdf)] [[Code](https://github.com/google-research/google-research/tree/master/routing_transformer)] ![Routing_Transformer](https://img.shields.io/badge/Routing_Transformer-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Image_Generation](https://img.shields.io/badge/Image_Generation-green) ![Clustering](https://img.shields.io/badge/Clustering-orange) ![Routing_Attention](https://img.shields.io/badge/Routing_Attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[5] Not all memories are created equal: Learning to forget by expiring**, ICML 2021  
  *Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela*  
  [[Paper](http://proceedings.mlr.press/v139/sukhbaatar21a/sukhbaatar21a.pdf)] [[Code](https://github.com/facebookresearch/transformer-sequential)] ![Expire--Span](https://img.shields.io/badge/Expire--Span-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Expire](https://img.shields.io/badge/Expire-orange) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Expire](https://img.shields.io/badge/Expire-yellow) ![Memory](https://img.shields.io/badge/Memory-yellow) 

- **[6] On the transformer growth for progressive bert training**, NAACL 2021  
  *Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei*  
  [[Paper](https://aclanthology.org/2021.naacl-main.406.pdf)] [[Code](https://github.com/google-research/google-research/tree/master/grow_bert)] ![CompoundGrow](https://img.shields.io/badge/CompoundGrow-blue) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![BERT_Pretraining](https://img.shields.io/badge/BERT_Pretraining-yellow) 

- **[7] Memory-efficient Transformers via Top-$ k $ Attention**, SUSTAINLP 2021  
  *Gupta, Ankit and Dar, Guy and Goodman, Shaya and Ciprut, David and Berant, Jonathan*  
  [[Paper](https://aclanthology.org/2021.sustainlp-1.5.pdf)] [[Code](https://github.com/ag1988/top_k_attention)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Query_chunking](https://img.shields.io/badge/Query_chunking-orange) ![Top--k_attention](https://img.shields.io/badge/Top--k_attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[8] Magic pyramid: Accelerating inference with early exiting and token pruning**, NeurIPS-ENLSP 2021  
  *He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul*  
  [[Paper](https://neurips2021-nlp.github.io/papers/21/CameraReady/magic_pyramid.pdf)] ![Magic_Pyramid](https://img.shields.io/badge/Magic_Pyramid-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Similarity_Detection](https://img.shields.io/badge/Similarity_Detection-green) ![Topic_Identification_](https://img.shields.io/badge/Topic_Identification_-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Early_Existing](https://img.shields.io/badge/Early_Existing-yellow) ![Token_Pruning](https://img.shields.io/badge/Token_Pruning-yellow) 

- **[9] El-attention: Memory efficient lossless attention for generation**, ICML 2021  
  *Yan, Yu and Chen, Jiusheng and Qi, Weizhen and Bhendawade, Nikhil and Gong, Yeyun and Duan, Nan and Zhang, Ruofei*  
  [[Paper](https://proceedings.mlr.press/v139/yan21a/yan21a.pdf)] [[Code](https://github.com/microsoft/fastseq)] ![EL--Attention](https://img.shields.io/badge/EL--Attention-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Encoder--Decoder_Attention](https://img.shields.io/badge/Encoder--Decoder_Attention-yellow) ![Self--Attention](https://img.shields.io/badge/Self--Attention-yellow) 

## 2022

- **[1] Learned Token Pruning for Transformers**, KDD 2022  
  *Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt*  
  [[Paper](https://dl.acm.org/doi/pdf/10.1145/3534678.3539260)] [[Code](https://github.com/kssteven418/LTP)] ![LTP](https://img.shields.io/badge/LTP-blue) ![Sentence_Similarity](https://img.shields.io/badge/Sentence_Similarity-green) ![Classification](https://img.shields.io/badge/Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Learnable_Threshold](https://img.shields.io/badge/Learnable_Threshold-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[2] Block-skim: Efficient question answering for transformer**, AAAI  2022  
  *Guan, Yue and Li, Zhengyi and Lin, Zhouhan and Zhu, Yuhao and Leng, Jingwen and Guo, Minyi*  
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/21316/21065)] [[Code](https://github.com/ChandlerGuan/blockskim)] ![Block--Skim](https://img.shields.io/badge/Block--Skim-blue) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Conv](https://img.shields.io/badge/Conv-orange) ![Skimming](https://img.shields.io/badge/Skimming-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[3] Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models**, ACL 2022  
  *Wingate, David and Shoeybi, Mohammad and Sorensen, Taylor*  
  [[Paper](https://aclanthology.org/2022.findings-emnlp.412.pdf)] [[Code](https://github.com/BYU-PCCL/prompt-compression-contrastive-coding)] ![Toxicity_reduction](https://img.shields.io/badge/Toxicity_reduction-green) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Prompt_Compression](https://img.shields.io/badge/Prompt_Compression-yellow) ![Exploration](https://img.shields.io/badge/Exploration-yellow) 

- **[4] Memorizing transformers**, ICLR 2022  
  *Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy*  
  [[Paper](https://openreview.net/pdf?id=TrjbxzRcnf-)] [[Code](https://github.com/lucidrains/memorizing-transformers-pytorch)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Caching](https://img.shields.io/badge/Caching-orange) ![kNN_Attention](https://img.shields.io/badge/kNN_Attention-orange) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[5] Transkimmer: Transformer learns to layer-wise skim**, ACL 2022  
  *Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi*  
  [[Paper](https://aclanthology.org/2022.acl-long.502.pdf)] [[Code](https://github.com/chandlerguan/transkimmer)] ![Transkimmer](https://img.shields.io/badge/Transkimmer-blue) ![QA](https://img.shields.io/badge/QA-green) ![Sequence--leval_Classification](https://img.shields.io/badge/Sequence--leval_Classification-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Skimming](https://img.shields.io/badge/Skimming-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[6] Adapler: Speeding up inference by adaptive length reduction**, ACL 2022  
  *Modarressi, Ali and Mohebbi, Hosein and Pilehvar, Mohammad Taher*  
  [[Paper](https://aclanthology.org/2022.acl-long.1.pdf)] [[Code](https://github.com/amodaresi/AdapLeR)] ![AdapLeR](https://img.shields.io/badge/AdapLeR-blue) ![Topic_Classification](https://img.shields.io/badge/Topic_Classification-green) ![Sentiment_Classification](https://img.shields.io/badge/Sentiment_Classification-green) ![Knowledge_Extraction](https://img.shields.io/badge/Knowledge_Extraction-green) ![QA](https://img.shields.io/badge/QA-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[7] Fine-and coarse-granularity hybrid self-attention for efficient bert**, ACL 2022  
  *Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong*  
  [[Paper](https://aclanthology.org/2022.acl-long.330.pdf)] [[Code](https://github.com/pierre-zhao/FCA-BERT)] ![FCA--BERT](https://img.shields.io/badge/FCA--BERT-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Similarity](https://img.shields.io/badge/Similarity-green) ![NLI](https://img.shields.io/badge/NLI-green) ![Sentiment](https://img.shields.io/badge/Sentiment-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

## 2023

- **[1] Efficient long-text understanding with short-text models**, TACL 2023  
  *Ivgi, Maor and Shaham, Uri and Berant, Jonathan*  
  [[Paper](https://aclanthology.org/2023.tacl-1.17.pdf)] [[Code](https://github.com/Mivg/SLED)] ![SLED](https://img.shields.io/badge/SLED-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow) 

- **[2] Extensible prompts for language models on Zero-shot Language Style Customization**, NeurIPS 2023  
  *Tao Ge and Jing Hu and Li Dong and Shaoguang Mao and Yan Xia and Xun Wang and Si-Qing Chen and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=lRxpVfDMzz)] ![Language_Style_Customization](https://img.shields.io/badge/Language_Style_Customization-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[3] Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification**, EMNLP 2023  
  *Yun, Jungmin and Kim, Mihyeon and Kim, Youngbin*  
  [[Paper](https://aclanthology.org/2023.findings-emnlp.909.pdf)] ![Document_Classification](https://img.shields.io/badge/Document_Classification-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Fuzzy--based_Token_Pruning](https://img.shields.io/badge/Fuzzy--based_Token_Pruning-orange) ![Token_Combining](https://img.shields.io/badge/Token_Combining-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[4] Compressing context to enhance inference efficiency of large language models**, EMNLP 2023  
  *Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.391.pdf)] [[Code](https://github.com/liyucheng09/Selective_Context)] ![Selective_Context](https://img.shields.io/badge/Selective_Context-blue) ![QA](https://img.shields.io/badge/QA-green) ![Summarization](https://img.shields.io/badge/Summarization-green) ![Conversation](https://img.shields.io/badge/Conversation-green) ![Self--Information](https://img.shields.io/badge/Self--Information-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) ![LLM_Generation](https://img.shields.io/badge/LLM_Generation-yellow) 

- **[5] Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning**, ACL 2023  
  *Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng*  
  [[Paper](https://aclanthology.org/2023.acl-long.172.pdf)] [[Code](https://github.com/fanyin3639/Rethinking-instruction-effectiveness)] ![STDC](https://img.shields.io/badge/STDC-blue) ![Classification](https://img.shields.io/badge/Classification-green) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Syntax--guided](https://img.shields.io/badge/Syntax--guided-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[6] Llmlingua: Compressing prompts for accelerated inference of large language models**, EMNLP 2023  
  *Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.825.pdf)] [[Code](https://aka.ms/LLMLingua)] ![LLMLingua_](https://img.shields.io/badge/LLMLingua_-blue) ![Classification](https://img.shields.io/badge/Classification-green) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[7] Learning to compress prompts with gist tokens**, NeurIPS 2023  
  *Mu, Jesse and Li, Xiang and Goodman, Noah*  
  [[Paper](https://openreview.net/pdf?id=2DtxPCL3T5)] [[Code](https://github.com/jayelm/gisting)] ![gisting](https://img.shields.io/badge/gisting-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[8] Adapting language models to compress contexts**, EMNLP 2023  
  *Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.232.pdf)] [[Code](https://github.com/princeton-nlp/AutoCompressors)] ![AutoCompressors](https://img.shields.io/badge/AutoCompressors-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[9] Inference with reference: Lossless acceleration of large language models**, arXiv 2023  
  *Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu*  
  [[Paper](https://arxiv.org/pdf/2304.04487)] ![LLMA](https://img.shields.io/badge/LLMA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Copy](https://img.shields.io/badge/Copy-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[10] Efficient prompting via dynamic in-context learning**, arXiv 2023  
  *Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya*  
  [[Paper](https://arxiv.org/pdf/2305.11170)] ![DynaICL](https://img.shields.io/badge/DynaICL-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[11] Nugget: Neural agglomerative embeddings of text**, ICML  2023  
  *Qin, Guanghui and Van Durme, Benjamin*  
  [[Paper](https://arxiv.org/pdf/2310.01732)] [[Code](https://github.com/hiaoxui/nugget)] ![Nugget](https://img.shields.io/badge/Nugget-blue) ![Similarity](https://img.shields.io/badge/Similarity-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Nugget_Generator](https://img.shields.io/badge/Nugget_Generator-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[12] Unlimiformer: Long-range transformers with unlimited length input**, NeurIPS 2023  
  *Amanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley*  
  [[Paper](https://openreview.net/pdf?id=lJWUJWLCJo)] [[Code](https://github.com/abertsch72/unlimiformer)] ![Unlimiformer](https://img.shields.io/badge/Unlimiformer-blue) ![Similarity](https://img.shields.io/badge/Similarity-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Top--k_retrieval](https://img.shields.io/badge/Top--k_retrieval-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow) 

- **[13] Walking down the memory maze: Beyond context limit through interactive reading**, arXiv 2023  
  *Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli*  
  [[Paper](https://arxiv.org/pdf/2310.05029)] ![MemWalker](https://img.shields.io/badge/MemWalker-blue) ![Similarity](https://img.shields.io/badge/Similarity-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Divide_and_Donquer](https://img.shields.io/badge/Divide_and_Donquer-orange) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[14] Sparse token transformer with attention back tracking**, ICLR 2023  
  *Heejun Lee and Minki Kang and Youngwan Lee and Sung Ju Hwang*  
  [[Paper](https://openreview.net/pdf?id=VV0hSE8AxCw)] [[Code](https://github.com/gmlwns2000/sttabt)] ![STTABT](https://img.shields.io/badge/STTABT-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Attention_Approximation_Network](https://img.shields.io/badge/Attention_Approximation_Network-purple) ![Attention_Back--tracking](https://img.shields.io/badge/Attention_Back--tracking-orange) ![Concrete_Masking](https://img.shields.io/badge/Concrete_Masking-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[15] Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time**, NeurIPS 2023  
  *Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava*  
  [[Paper](https://openreview.net/pdf?id=JZfg6wGi6g)] ![Scissorhands](https://img.shields.io/badge/Scissorhands-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[16] H2o: Heavy-hitter oracle for efficient generative inference of large language models**, NeurIPS 2023  
  *Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen*  
  [[Paper](https://openreview.net/pdf?id=RkRrPp7GKO)] [[Code](https://github.com/FMInference/H2O)] ![H2O](https://img.shields.io/badge/H2O-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[17] Landmark attention: Random-access infinite context length for transformers**, NeurIPS 2023  
  *Amirkeivan Mohtashami and Martin Jaggi*  
  [[Paper](https://openreview.net/pdf?id=7eHn64wOVy)] [[Code](https://github.com/epfml/landmark-attention/)] ![Landmark_attention](https://img.shields.io/badge/Landmark_attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[18] Optimizing retrieval-augmented reader models via token elimination**, EMNLP 2023  
  *Berchansky, Moshe and Izsak, Peter and Caciularu, Avi and Dagan, Ido and Wasserblat, Moshe*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.93.pdf)] [[Code](https://github.com/IntelLabs/token_elimination)] ![Token_Elimination](https://img.shields.io/badge/Token_Elimination-blue) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[19] Efficient transformers with dynamic token pooling**, ACL 2023  
  *Nawrot, Piotr and Chorowski, Jan and Lancucki, Adrian and Ponti, Edoardo Maria*  
  [[Paper](https://aclanthology.org/2023.acl-long.353.pdf)] [[Code](https://github.com/PiotrNawrot/dynamic-pooling)] ![Dynamic_Token_Pooling](https://img.shields.io/badge/Dynamic_Token_Pooling-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

## 2024

- **[1] Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression**, ACL 2024  
  *Jiang, Huiqiang  and Wu, Qianhui  and Luo, Xufang  and Li, Dongsheng  and Lin, Chin-Yew  and Yang, Yuqing  and Qiu, Lili*  
  [[Paper](https://aclanthology.org/2024.acl-long.91.pdf)] [[Code](https://aka.ms/LLMLingua)] ![LongLLMLingua](https://img.shields.io/badge/LongLLMLingua-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Drop--and--Restore](https://img.shields.io/badge/Drop--and--Restore-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[2] In-context autoencoder for context compression in a large language model**, ICLR 2024  
  *Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=uREj4ZuGJE)] [[Code](https://github.com/getao/icae)] ![ICAE](https://img.shields.io/badge/ICAE-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--based](https://img.shields.io/badge/Training--based-pink) 

- **[3] Efficient streaming language models with attention sinks**, ICLR 2024  
  *Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis*  
  [[Paper](https://openreview.net/pdf?id=NG7sS51zVF)] [[Code](https://github.com/mit-han-lab/streaming-llm)] ![StreamingLLM](https://img.shields.io/badge/StreamingLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[4] Model tells you what to discard: Adaptive kv cache compression for llms**, ICLR 2024  
  *Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao*  
  [[Paper](https://openreview.net/pdf?id=uNrFpDPMyo)] [[Code](https://github.com/machilusZ/FastGen)] ![FastGen](https://img.shields.io/badge/FastGen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Policy](https://img.shields.io/badge/Policy-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[5] Sparq attention: Bandwidth-efficient llm inference**, ICLR-ME-FoMo 2024  
  *Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr*  
  [[Paper](https://openreview.net/pdf?id=Ue8EHzaFI4)] [[Code](https://github.com/graphcore-research/llm-inference-research/tree/2024-01-paper)] ![SparQ_attention](https://img.shields.io/badge/SparQ_attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Sparsity](https://img.shields.io/badge/Sparsity-orange) ![Training--free](https://img.shields.io/badge/Training--free-pink) 

- **[6] FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing**, 2024  
  

- **[7] Discrete prompt compression with reinforcement learning**, 2024  
  

- **[8] Hierarchical context merging: Better long context understanding for pre-trained LLMs**, 2024  
  

- **[9] LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference**, 2024  
  

- **[10] Dynamic context pruning for efficient and interpretable autoregressive transformers**, 2024  
  

- **[11] You only cache once: Decoder-decoder architectures for language models**, 2024  
  

- **[12] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**, 2024  
  

- **[13] Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression**, 2024  
  

- **[14] Long-context language modeling with parallel context encoding**, 2024  
  

- **[15] LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models**, 2024  
  

- **[16] Transformers are multi-state rnns**, 2024  
  

- **[17] PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference**, 2024  
  

- **[18] Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling**, 2024  
  

- **[19] Keyformer: Kv cache reduction through key tokens selection for efficient generative inference**, 2024  
  

- **[20] Finch: Prompt-guided key-value cache compression**, 2024  
  

- **[21] A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder**, 2024  
  

- **[22] Think: Thinner key cache by query-driven pruning**, 2024  
  

- **[23] Razorattention: Efficient kv cache compression through retrieval heads**, 2024  
  

- **[24] SirLLM: Streaming infinite retentive LLM**, 2024  
  

- **[25] A Simple and Effective $ L_2 $ Norm-Based Strategy for KV Cache Compression**, 2024  
  

- **[26] Dynamic memory compression: Retrofitting llms for accelerated inference**, 2024  
  

- **[27] Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks**, 2024  
  

- **[28] Anchor-based large language models**, 2024  
  

- **[29] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption**, 2024  
  

- **[30] SCBench: A KV Cache-Centric Analysis of Long-Context Methods**, 2024  
  

- **[31] On the efficacy of eviction policy for key-value constrained generative language model inference**, 2024  
  

- **[32] Snapkv: Llm knows what you are looking for before generation**, 2024  
  

- **[33] SubGen: Token Generation in Sublinear Time and Memory**, 2024  
  

- **[34] Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning**, 2024  
  

- **[35] Sequence can Secretly Tell You What to Discard**, 2024  
  

- **[36] Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters**, 2024  
  

- **[37] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models**, 2024  
  

- **[38] Cam: Cache merging for memory-efficient llms inference**, 2024  
  

- **[39] D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models**, 2024  
  

- **[40] Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration**, 2024  
  

- **[41] Sglang: Efficient execution of structured language model programs**, 2024  
  

- **[42] Prompt cache: Modular attention reuse for low-latency inference**, 2024  
  

- **[43] Cascade inference: Memory bandwidth efficient shared prefix batch decoding**, 2024  
  

- **[44] Hydragen: High-Throughput LLM Inference with Shared Prefixes**, 2024  
  

- **[45] RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation**, 2024  
  

- **[46] CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion**, 2024  
  

- **[47] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference**, 2024  
  

- **[48] Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference**, 2024  
  

- **[49] Massive activations in large language models**, 2024  
  

- **[50] Unifying KV Cache Compression for Large Language Models with LeanKV**, 2024  
  

- **[51] More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression**, 2024  
  

- **[52] LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management**, 2024  
  

- **[53] KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head**, 2024  
  

- **[54] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty**, 2024  
  

- **[55] UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference**, 2024  
  

- **[56] KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing**, 2024  
  

- **[57] LoCoCo: Dropping In Convolutions for Long Context Compression**, 2024  
  

- **[58] SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction**, 2024  
  

- **[59] EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance**, 2024  
  

- **[60] ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression**, 2024  
  

- **[61] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads**, 2024  
  

- **[62] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization**, 2024  
  

- **[63] LSH-E Tells You What To Discard: An Adaptive Locality-Sensitive Strategy for KV Cache Compression**, 2024  
  

- **[64] Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression**, 2024  
  

- **[65] LoMA: Lossless Compressed Memory Attention**, 2024  
  

- **[66] KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches**, 2024  
  

- **[67] NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time**, 2024  
  

- **[68] TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection**, 2024  
  

- **[69] ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction**, 2024  
  

- **[70] Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache**, 2024  
  

- **[71] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management**, 2024  
  

- **[72] XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference**, 2024  
  

- **[73] In-context KV-Cache Eviction for LLMs via Attention-Gate**, 2024  
  

- **[74] Recycled Attention: Efficient inference for long-context language models**, 2024  
  

- **[75] MagicPIG: LSH Sampling for Efficient LLM Generation**, 2024  
  

- **[76] Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache**, 2024  
  

- **[77] When Attention Sink Emerges in Language Models: An Empirical View**, 2024  
  

- **[78] Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference**, 2024  
  

- **[79] SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**, 2024  
  

- **[80] A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference**, 2024  
  

- **[81] LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts**, 2024  
  

- **[82] Efficient Sparse Attention needs Adaptive Token Release**, 2024  
  

- **[83] Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling**, 2024  
  

